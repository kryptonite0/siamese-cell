{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightGBM version 2.2.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from bayes_opt import BayesianOptimization\n",
    "from lightgbm import LGBMClassifier\n",
    "import numpy as np\n",
    "\n",
    "import settings_model\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_rows', 50000)\n",
    "pd.set_option('display.max_columns', 50000)\n",
    "\n",
    "print('lightGBM version', lgb.__version__)\n",
    "\n",
    "# model_id = \"HUVEC_20190810_202545\"\n",
    "model_id = \"HEPG2_20190811_142600\"\n",
    "output_dir = os.path.join(settings_model.root_path, \"models\", \"siamese-cell\",\n",
    "                          f\"{model_id}\", \"emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (86220, 129)\n",
      "valid (992, 129)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22570</th>\n",
       "      <td>1.115120</td>\n",
       "      <td>-0.812163</td>\n",
       "      <td>0.999520</td>\n",
       "      <td>0.473936</td>\n",
       "      <td>-0.111045</td>\n",
       "      <td>0.401779</td>\n",
       "      <td>0.298741</td>\n",
       "      <td>0.027764</td>\n",
       "      <td>-0.198880</td>\n",
       "      <td>0.195782</td>\n",
       "      <td>-0.203637</td>\n",
       "      <td>1.481260</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>1.206008</td>\n",
       "      <td>0.135087</td>\n",
       "      <td>-0.701905</td>\n",
       "      <td>0.401269</td>\n",
       "      <td>0.770378</td>\n",
       "      <td>-0.384910</td>\n",
       "      <td>1.051266</td>\n",
       "      <td>-0.456784</td>\n",
       "      <td>-0.002182</td>\n",
       "      <td>0.541055</td>\n",
       "      <td>0.248358</td>\n",
       "      <td>-1.100184</td>\n",
       "      <td>-0.175203</td>\n",
       "      <td>-0.208046</td>\n",
       "      <td>-0.867965</td>\n",
       "      <td>0.499489</td>\n",
       "      <td>0.186703</td>\n",
       "      <td>0.184205</td>\n",
       "      <td>0.983467</td>\n",
       "      <td>0.171228</td>\n",
       "      <td>-0.473962</td>\n",
       "      <td>-0.067935</td>\n",
       "      <td>-0.746544</td>\n",
       "      <td>1.021289</td>\n",
       "      <td>1.183232</td>\n",
       "      <td>0.386953</td>\n",
       "      <td>-0.641643</td>\n",
       "      <td>0.311075</td>\n",
       "      <td>-0.683508</td>\n",
       "      <td>0.151159</td>\n",
       "      <td>-0.835884</td>\n",
       "      <td>0.314692</td>\n",
       "      <td>0.222327</td>\n",
       "      <td>-0.951957</td>\n",
       "      <td>-0.330503</td>\n",
       "      <td>-0.954284</td>\n",
       "      <td>-0.907088</td>\n",
       "      <td>-0.381297</td>\n",
       "      <td>-0.568039</td>\n",
       "      <td>0.165597</td>\n",
       "      <td>0.070025</td>\n",
       "      <td>0.762621</td>\n",
       "      <td>0.761588</td>\n",
       "      <td>0.041996</td>\n",
       "      <td>-0.491939</td>\n",
       "      <td>0.266267</td>\n",
       "      <td>-0.043106</td>\n",
       "      <td>0.338545</td>\n",
       "      <td>-0.468541</td>\n",
       "      <td>-0.267147</td>\n",
       "      <td>0.514451</td>\n",
       "      <td>-0.990372</td>\n",
       "      <td>0.559909</td>\n",
       "      <td>0.069218</td>\n",
       "      <td>0.906935</td>\n",
       "      <td>-0.240570</td>\n",
       "      <td>-0.102345</td>\n",
       "      <td>0.305328</td>\n",
       "      <td>0.917522</td>\n",
       "      <td>1.085336</td>\n",
       "      <td>-0.495803</td>\n",
       "      <td>0.947908</td>\n",
       "      <td>-0.769751</td>\n",
       "      <td>-1.088021</td>\n",
       "      <td>0.469260</td>\n",
       "      <td>0.305072</td>\n",
       "      <td>-0.641360</td>\n",
       "      <td>0.150511</td>\n",
       "      <td>0.068960</td>\n",
       "      <td>1.043315</td>\n",
       "      <td>-0.418300</td>\n",
       "      <td>1.085144</td>\n",
       "      <td>-0.928959</td>\n",
       "      <td>-0.167872</td>\n",
       "      <td>0.812571</td>\n",
       "      <td>-0.183837</td>\n",
       "      <td>-0.667301</td>\n",
       "      <td>0.518967</td>\n",
       "      <td>-0.783982</td>\n",
       "      <td>0.553192</td>\n",
       "      <td>0.482868</td>\n",
       "      <td>0.470178</td>\n",
       "      <td>0.178276</td>\n",
       "      <td>0.539762</td>\n",
       "      <td>0.035636</td>\n",
       "      <td>-0.467180</td>\n",
       "      <td>0.499896</td>\n",
       "      <td>-0.580248</td>\n",
       "      <td>0.625731</td>\n",
       "      <td>0.836909</td>\n",
       "      <td>-0.334144</td>\n",
       "      <td>0.471594</td>\n",
       "      <td>0.621171</td>\n",
       "      <td>-0.150757</td>\n",
       "      <td>-0.340110</td>\n",
       "      <td>0.281607</td>\n",
       "      <td>-0.465407</td>\n",
       "      <td>-0.528346</td>\n",
       "      <td>0.132581</td>\n",
       "      <td>-0.491218</td>\n",
       "      <td>0.739732</td>\n",
       "      <td>0.371598</td>\n",
       "      <td>-0.109128</td>\n",
       "      <td>-0.263545</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.851574</td>\n",
       "      <td>0.607503</td>\n",
       "      <td>0.562765</td>\n",
       "      <td>0.457904</td>\n",
       "      <td>0.179819</td>\n",
       "      <td>-0.511334</td>\n",
       "      <td>-0.520777</td>\n",
       "      <td>0.024689</td>\n",
       "      <td>-0.189900</td>\n",
       "      <td>-0.254532</td>\n",
       "      <td>842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16685</th>\n",
       "      <td>0.942946</td>\n",
       "      <td>-0.648891</td>\n",
       "      <td>0.856448</td>\n",
       "      <td>0.417749</td>\n",
       "      <td>-0.028715</td>\n",
       "      <td>0.315346</td>\n",
       "      <td>0.253220</td>\n",
       "      <td>0.029043</td>\n",
       "      <td>-0.127225</td>\n",
       "      <td>0.138472</td>\n",
       "      <td>-0.150499</td>\n",
       "      <td>1.254624</td>\n",
       "      <td>0.063846</td>\n",
       "      <td>1.008367</td>\n",
       "      <td>0.133532</td>\n",
       "      <td>-0.587394</td>\n",
       "      <td>0.306522</td>\n",
       "      <td>0.590236</td>\n",
       "      <td>-0.309718</td>\n",
       "      <td>0.814211</td>\n",
       "      <td>-0.454224</td>\n",
       "      <td>-0.044430</td>\n",
       "      <td>0.485063</td>\n",
       "      <td>0.254149</td>\n",
       "      <td>-0.977464</td>\n",
       "      <td>-0.069657</td>\n",
       "      <td>-0.162968</td>\n",
       "      <td>-0.706081</td>\n",
       "      <td>0.385089</td>\n",
       "      <td>0.159863</td>\n",
       "      <td>0.103334</td>\n",
       "      <td>0.869885</td>\n",
       "      <td>0.189985</td>\n",
       "      <td>-0.437686</td>\n",
       "      <td>-0.103969</td>\n",
       "      <td>-0.707402</td>\n",
       "      <td>0.890293</td>\n",
       "      <td>1.008038</td>\n",
       "      <td>0.294505</td>\n",
       "      <td>-0.596239</td>\n",
       "      <td>0.295053</td>\n",
       "      <td>-0.581890</td>\n",
       "      <td>0.074221</td>\n",
       "      <td>-0.689581</td>\n",
       "      <td>0.240678</td>\n",
       "      <td>0.243086</td>\n",
       "      <td>-0.774149</td>\n",
       "      <td>-0.269381</td>\n",
       "      <td>-0.845396</td>\n",
       "      <td>-0.747802</td>\n",
       "      <td>-0.357692</td>\n",
       "      <td>-0.575798</td>\n",
       "      <td>0.075748</td>\n",
       "      <td>0.053659</td>\n",
       "      <td>0.603290</td>\n",
       "      <td>0.588036</td>\n",
       "      <td>-0.044867</td>\n",
       "      <td>-0.231689</td>\n",
       "      <td>0.222319</td>\n",
       "      <td>-0.028392</td>\n",
       "      <td>0.322678</td>\n",
       "      <td>-0.388349</td>\n",
       "      <td>-0.233056</td>\n",
       "      <td>0.504491</td>\n",
       "      <td>-0.827719</td>\n",
       "      <td>0.423305</td>\n",
       "      <td>0.022377</td>\n",
       "      <td>0.745191</td>\n",
       "      <td>-0.158106</td>\n",
       "      <td>-0.051284</td>\n",
       "      <td>0.236679</td>\n",
       "      <td>0.785260</td>\n",
       "      <td>0.909442</td>\n",
       "      <td>-0.423034</td>\n",
       "      <td>0.779821</td>\n",
       "      <td>-0.655471</td>\n",
       "      <td>-0.895765</td>\n",
       "      <td>0.324838</td>\n",
       "      <td>0.283415</td>\n",
       "      <td>-0.495799</td>\n",
       "      <td>0.173841</td>\n",
       "      <td>0.071632</td>\n",
       "      <td>0.870188</td>\n",
       "      <td>-0.370303</td>\n",
       "      <td>0.857812</td>\n",
       "      <td>-0.800465</td>\n",
       "      <td>-0.098202</td>\n",
       "      <td>0.706219</td>\n",
       "      <td>-0.139726</td>\n",
       "      <td>-0.653560</td>\n",
       "      <td>0.447379</td>\n",
       "      <td>-0.664467</td>\n",
       "      <td>0.430281</td>\n",
       "      <td>0.468289</td>\n",
       "      <td>0.429375</td>\n",
       "      <td>0.127178</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>-0.075853</td>\n",
       "      <td>-0.353152</td>\n",
       "      <td>0.462548</td>\n",
       "      <td>-0.459914</td>\n",
       "      <td>0.565993</td>\n",
       "      <td>0.650294</td>\n",
       "      <td>-0.299079</td>\n",
       "      <td>0.456827</td>\n",
       "      <td>0.524460</td>\n",
       "      <td>-0.140988</td>\n",
       "      <td>-0.250238</td>\n",
       "      <td>0.290855</td>\n",
       "      <td>-0.331692</td>\n",
       "      <td>-0.456499</td>\n",
       "      <td>0.110501</td>\n",
       "      <td>-0.394977</td>\n",
       "      <td>0.630778</td>\n",
       "      <td>0.327174</td>\n",
       "      <td>-0.140782</td>\n",
       "      <td>-0.256002</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.696451</td>\n",
       "      <td>0.415953</td>\n",
       "      <td>0.457629</td>\n",
       "      <td>0.395805</td>\n",
       "      <td>0.172674</td>\n",
       "      <td>-0.452176</td>\n",
       "      <td>-0.466752</td>\n",
       "      <td>0.021225</td>\n",
       "      <td>-0.216684</td>\n",
       "      <td>-0.218758</td>\n",
       "      <td>355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123440</th>\n",
       "      <td>1.121289</td>\n",
       "      <td>-0.803799</td>\n",
       "      <td>1.045861</td>\n",
       "      <td>0.504945</td>\n",
       "      <td>-0.051864</td>\n",
       "      <td>0.485084</td>\n",
       "      <td>0.399641</td>\n",
       "      <td>0.072972</td>\n",
       "      <td>-0.139932</td>\n",
       "      <td>0.116327</td>\n",
       "      <td>-0.175476</td>\n",
       "      <td>1.567504</td>\n",
       "      <td>0.113902</td>\n",
       "      <td>1.208327</td>\n",
       "      <td>0.097454</td>\n",
       "      <td>-0.642899</td>\n",
       "      <td>0.388071</td>\n",
       "      <td>0.740143</td>\n",
       "      <td>-0.353415</td>\n",
       "      <td>0.988833</td>\n",
       "      <td>-0.552584</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.519598</td>\n",
       "      <td>0.220283</td>\n",
       "      <td>-1.153672</td>\n",
       "      <td>-0.086348</td>\n",
       "      <td>-0.296566</td>\n",
       "      <td>-0.891800</td>\n",
       "      <td>0.575132</td>\n",
       "      <td>0.156740</td>\n",
       "      <td>0.151445</td>\n",
       "      <td>0.947815</td>\n",
       "      <td>0.176358</td>\n",
       "      <td>-0.513119</td>\n",
       "      <td>-0.141243</td>\n",
       "      <td>-0.855411</td>\n",
       "      <td>1.111049</td>\n",
       "      <td>1.181610</td>\n",
       "      <td>0.266485</td>\n",
       "      <td>-0.782169</td>\n",
       "      <td>0.305061</td>\n",
       "      <td>-0.756833</td>\n",
       "      <td>0.117234</td>\n",
       "      <td>-0.832389</td>\n",
       "      <td>0.225036</td>\n",
       "      <td>0.380719</td>\n",
       "      <td>-0.864756</td>\n",
       "      <td>-0.381530</td>\n",
       "      <td>-0.909695</td>\n",
       "      <td>-0.860609</td>\n",
       "      <td>-0.492003</td>\n",
       "      <td>-0.759491</td>\n",
       "      <td>0.094117</td>\n",
       "      <td>0.057793</td>\n",
       "      <td>0.702132</td>\n",
       "      <td>0.687047</td>\n",
       "      <td>-0.006143</td>\n",
       "      <td>-0.398009</td>\n",
       "      <td>0.303515</td>\n",
       "      <td>0.107709</td>\n",
       "      <td>0.446104</td>\n",
       "      <td>-0.495050</td>\n",
       "      <td>-0.243958</td>\n",
       "      <td>0.582464</td>\n",
       "      <td>-1.070652</td>\n",
       "      <td>0.549145</td>\n",
       "      <td>0.059315</td>\n",
       "      <td>0.868515</td>\n",
       "      <td>-0.249779</td>\n",
       "      <td>-0.150307</td>\n",
       "      <td>0.288268</td>\n",
       "      <td>0.953198</td>\n",
       "      <td>1.076480</td>\n",
       "      <td>-0.557308</td>\n",
       "      <td>1.064692</td>\n",
       "      <td>-0.803946</td>\n",
       "      <td>-1.160680</td>\n",
       "      <td>0.444875</td>\n",
       "      <td>0.233066</td>\n",
       "      <td>-0.585349</td>\n",
       "      <td>0.186286</td>\n",
       "      <td>0.070056</td>\n",
       "      <td>1.118312</td>\n",
       "      <td>-0.422528</td>\n",
       "      <td>0.969558</td>\n",
       "      <td>-1.010211</td>\n",
       "      <td>-0.098995</td>\n",
       "      <td>0.764818</td>\n",
       "      <td>-0.184648</td>\n",
       "      <td>-0.706490</td>\n",
       "      <td>0.568750</td>\n",
       "      <td>-0.762300</td>\n",
       "      <td>0.539530</td>\n",
       "      <td>0.464918</td>\n",
       "      <td>0.484420</td>\n",
       "      <td>0.236719</td>\n",
       "      <td>0.562145</td>\n",
       "      <td>-0.236458</td>\n",
       "      <td>-0.464758</td>\n",
       "      <td>0.662213</td>\n",
       "      <td>-0.561075</td>\n",
       "      <td>0.655185</td>\n",
       "      <td>0.800641</td>\n",
       "      <td>-0.334941</td>\n",
       "      <td>0.421006</td>\n",
       "      <td>0.629312</td>\n",
       "      <td>-0.174035</td>\n",
       "      <td>-0.345552</td>\n",
       "      <td>0.307037</td>\n",
       "      <td>-0.442606</td>\n",
       "      <td>-0.579471</td>\n",
       "      <td>0.117530</td>\n",
       "      <td>-0.487585</td>\n",
       "      <td>0.723310</td>\n",
       "      <td>0.459936</td>\n",
       "      <td>-0.063990</td>\n",
       "      <td>-0.267776</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.861442</td>\n",
       "      <td>0.550744</td>\n",
       "      <td>0.593329</td>\n",
       "      <td>0.406331</td>\n",
       "      <td>0.187094</td>\n",
       "      <td>-0.481162</td>\n",
       "      <td>-0.555893</td>\n",
       "      <td>-0.037544</td>\n",
       "      <td>-0.190062</td>\n",
       "      <td>-0.358771</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132746</th>\n",
       "      <td>1.117417</td>\n",
       "      <td>-0.757460</td>\n",
       "      <td>0.942376</td>\n",
       "      <td>0.499040</td>\n",
       "      <td>-0.081432</td>\n",
       "      <td>0.380781</td>\n",
       "      <td>0.215095</td>\n",
       "      <td>0.068094</td>\n",
       "      <td>-0.216357</td>\n",
       "      <td>0.168529</td>\n",
       "      <td>-0.009598</td>\n",
       "      <td>1.463718</td>\n",
       "      <td>0.077280</td>\n",
       "      <td>1.200349</td>\n",
       "      <td>0.136644</td>\n",
       "      <td>-0.641120</td>\n",
       "      <td>0.443802</td>\n",
       "      <td>0.960702</td>\n",
       "      <td>-0.332269</td>\n",
       "      <td>1.157909</td>\n",
       "      <td>-0.418768</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>0.603946</td>\n",
       "      <td>0.343115</td>\n",
       "      <td>-0.831829</td>\n",
       "      <td>-0.259492</td>\n",
       "      <td>-0.152674</td>\n",
       "      <td>-0.754837</td>\n",
       "      <td>0.562401</td>\n",
       "      <td>0.196445</td>\n",
       "      <td>0.208154</td>\n",
       "      <td>0.930183</td>\n",
       "      <td>0.174315</td>\n",
       "      <td>-0.362330</td>\n",
       "      <td>-0.174343</td>\n",
       "      <td>-0.746526</td>\n",
       "      <td>1.024377</td>\n",
       "      <td>1.120853</td>\n",
       "      <td>0.412802</td>\n",
       "      <td>-0.643252</td>\n",
       "      <td>0.421478</td>\n",
       "      <td>-0.670171</td>\n",
       "      <td>0.190803</td>\n",
       "      <td>-0.731712</td>\n",
       "      <td>0.375393</td>\n",
       "      <td>0.244755</td>\n",
       "      <td>-0.931101</td>\n",
       "      <td>-0.324493</td>\n",
       "      <td>-0.965189</td>\n",
       "      <td>-0.913058</td>\n",
       "      <td>-0.375528</td>\n",
       "      <td>-0.599962</td>\n",
       "      <td>0.129909</td>\n",
       "      <td>0.137584</td>\n",
       "      <td>0.764300</td>\n",
       "      <td>0.757572</td>\n",
       "      <td>0.070390</td>\n",
       "      <td>-0.591878</td>\n",
       "      <td>0.294490</td>\n",
       "      <td>-0.020089</td>\n",
       "      <td>0.315416</td>\n",
       "      <td>-0.402238</td>\n",
       "      <td>-0.252954</td>\n",
       "      <td>0.480173</td>\n",
       "      <td>-1.034404</td>\n",
       "      <td>0.558247</td>\n",
       "      <td>0.183675</td>\n",
       "      <td>0.872689</td>\n",
       "      <td>-0.360356</td>\n",
       "      <td>-0.175384</td>\n",
       "      <td>0.289037</td>\n",
       "      <td>0.867522</td>\n",
       "      <td>0.984221</td>\n",
       "      <td>-0.450662</td>\n",
       "      <td>1.052955</td>\n",
       "      <td>-0.635692</td>\n",
       "      <td>-1.088268</td>\n",
       "      <td>0.407191</td>\n",
       "      <td>0.330033</td>\n",
       "      <td>-0.687119</td>\n",
       "      <td>0.054822</td>\n",
       "      <td>-0.060173</td>\n",
       "      <td>1.039292</td>\n",
       "      <td>-0.361207</td>\n",
       "      <td>1.045623</td>\n",
       "      <td>-0.849056</td>\n",
       "      <td>-0.216978</td>\n",
       "      <td>0.812861</td>\n",
       "      <td>-0.111585</td>\n",
       "      <td>-0.619886</td>\n",
       "      <td>0.581188</td>\n",
       "      <td>-0.719431</td>\n",
       "      <td>0.518276</td>\n",
       "      <td>0.367100</td>\n",
       "      <td>0.451986</td>\n",
       "      <td>0.147992</td>\n",
       "      <td>0.438494</td>\n",
       "      <td>0.064807</td>\n",
       "      <td>-0.545258</td>\n",
       "      <td>0.530154</td>\n",
       "      <td>-0.565555</td>\n",
       "      <td>0.525659</td>\n",
       "      <td>0.894190</td>\n",
       "      <td>-0.343964</td>\n",
       "      <td>0.383234</td>\n",
       "      <td>0.647721</td>\n",
       "      <td>-0.171504</td>\n",
       "      <td>-0.427961</td>\n",
       "      <td>0.227468</td>\n",
       "      <td>-0.462600</td>\n",
       "      <td>-0.574659</td>\n",
       "      <td>0.121805</td>\n",
       "      <td>-0.529194</td>\n",
       "      <td>0.756697</td>\n",
       "      <td>0.366743</td>\n",
       "      <td>-0.023275</td>\n",
       "      <td>-0.280163</td>\n",
       "      <td>0.002904</td>\n",
       "      <td>0.942097</td>\n",
       "      <td>0.539076</td>\n",
       "      <td>0.560519</td>\n",
       "      <td>0.435520</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>-0.549574</td>\n",
       "      <td>-0.412764</td>\n",
       "      <td>-0.055057</td>\n",
       "      <td>-0.248691</td>\n",
       "      <td>-0.359678</td>\n",
       "      <td>991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88076</th>\n",
       "      <td>0.789321</td>\n",
       "      <td>-0.477822</td>\n",
       "      <td>0.631190</td>\n",
       "      <td>0.336736</td>\n",
       "      <td>-0.114380</td>\n",
       "      <td>0.233961</td>\n",
       "      <td>0.086919</td>\n",
       "      <td>0.113713</td>\n",
       "      <td>-0.105168</td>\n",
       "      <td>0.171326</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.936521</td>\n",
       "      <td>0.018509</td>\n",
       "      <td>0.957564</td>\n",
       "      <td>0.142301</td>\n",
       "      <td>-0.556350</td>\n",
       "      <td>0.465833</td>\n",
       "      <td>0.813451</td>\n",
       "      <td>-0.281022</td>\n",
       "      <td>0.755103</td>\n",
       "      <td>-0.140946</td>\n",
       "      <td>-0.037548</td>\n",
       "      <td>0.373408</td>\n",
       "      <td>0.366028</td>\n",
       "      <td>-0.520116</td>\n",
       "      <td>-0.164377</td>\n",
       "      <td>-0.072388</td>\n",
       "      <td>-0.520891</td>\n",
       "      <td>0.369514</td>\n",
       "      <td>0.164744</td>\n",
       "      <td>0.123882</td>\n",
       "      <td>0.565505</td>\n",
       "      <td>0.194681</td>\n",
       "      <td>-0.176737</td>\n",
       "      <td>-0.141644</td>\n",
       "      <td>-0.483641</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>0.762127</td>\n",
       "      <td>0.362083</td>\n",
       "      <td>-0.426424</td>\n",
       "      <td>0.307815</td>\n",
       "      <td>-0.444355</td>\n",
       "      <td>0.054236</td>\n",
       "      <td>-0.464765</td>\n",
       "      <td>0.433715</td>\n",
       "      <td>0.192827</td>\n",
       "      <td>-0.659232</td>\n",
       "      <td>-0.197318</td>\n",
       "      <td>-0.694278</td>\n",
       "      <td>-0.631891</td>\n",
       "      <td>-0.352255</td>\n",
       "      <td>-0.428348</td>\n",
       "      <td>0.049521</td>\n",
       "      <td>0.028710</td>\n",
       "      <td>0.419170</td>\n",
       "      <td>0.625482</td>\n",
       "      <td>0.100326</td>\n",
       "      <td>-0.505123</td>\n",
       "      <td>0.227617</td>\n",
       "      <td>-0.046826</td>\n",
       "      <td>0.304113</td>\n",
       "      <td>-0.267223</td>\n",
       "      <td>-0.273584</td>\n",
       "      <td>0.283153</td>\n",
       "      <td>-0.827209</td>\n",
       "      <td>0.403577</td>\n",
       "      <td>0.145685</td>\n",
       "      <td>0.603928</td>\n",
       "      <td>-0.232856</td>\n",
       "      <td>-0.145204</td>\n",
       "      <td>0.172127</td>\n",
       "      <td>0.526596</td>\n",
       "      <td>0.688882</td>\n",
       "      <td>-0.250277</td>\n",
       "      <td>0.660606</td>\n",
       "      <td>-0.325578</td>\n",
       "      <td>-0.706362</td>\n",
       "      <td>0.159460</td>\n",
       "      <td>0.271579</td>\n",
       "      <td>-0.558479</td>\n",
       "      <td>0.044891</td>\n",
       "      <td>-0.069888</td>\n",
       "      <td>0.651918</td>\n",
       "      <td>-0.311379</td>\n",
       "      <td>0.764495</td>\n",
       "      <td>-0.656329</td>\n",
       "      <td>-0.085217</td>\n",
       "      <td>0.496590</td>\n",
       "      <td>-0.094138</td>\n",
       "      <td>-0.396593</td>\n",
       "      <td>0.315139</td>\n",
       "      <td>-0.491023</td>\n",
       "      <td>0.301098</td>\n",
       "      <td>0.253831</td>\n",
       "      <td>0.315546</td>\n",
       "      <td>0.191494</td>\n",
       "      <td>0.312049</td>\n",
       "      <td>-0.006195</td>\n",
       "      <td>-0.323823</td>\n",
       "      <td>0.325932</td>\n",
       "      <td>-0.451024</td>\n",
       "      <td>0.278639</td>\n",
       "      <td>0.519498</td>\n",
       "      <td>-0.287530</td>\n",
       "      <td>0.275689</td>\n",
       "      <td>0.440547</td>\n",
       "      <td>-0.127253</td>\n",
       "      <td>-0.225619</td>\n",
       "      <td>0.151446</td>\n",
       "      <td>-0.322834</td>\n",
       "      <td>-0.445154</td>\n",
       "      <td>0.105608</td>\n",
       "      <td>-0.295885</td>\n",
       "      <td>0.657359</td>\n",
       "      <td>0.229650</td>\n",
       "      <td>0.060102</td>\n",
       "      <td>-0.212083</td>\n",
       "      <td>-0.069438</td>\n",
       "      <td>0.725006</td>\n",
       "      <td>0.269235</td>\n",
       "      <td>0.441460</td>\n",
       "      <td>0.249703</td>\n",
       "      <td>0.171428</td>\n",
       "      <td>-0.435611</td>\n",
       "      <td>-0.292266</td>\n",
       "      <td>-0.065743</td>\n",
       "      <td>-0.147618</td>\n",
       "      <td>-0.175354</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "22570   1.115120 -0.812163  0.999520  0.473936 -0.111045  0.401779  0.298741   \n",
       "16685   0.942946 -0.648891  0.856448  0.417749 -0.028715  0.315346  0.253220   \n",
       "123440  1.121289 -0.803799  1.045861  0.504945 -0.051864  0.485084  0.399641   \n",
       "132746  1.117417 -0.757460  0.942376  0.499040 -0.081432  0.380781  0.215095   \n",
       "88076   0.789321 -0.477822  0.631190  0.336736 -0.114380  0.233961  0.086919   \n",
       "\n",
       "               7         8         9        10        11        12        13  \\\n",
       "22570   0.027764 -0.198880  0.195782 -0.203637  1.481260  0.037662  1.206008   \n",
       "16685   0.029043 -0.127225  0.138472 -0.150499  1.254624  0.063846  1.008367   \n",
       "123440  0.072972 -0.139932  0.116327 -0.175476  1.567504  0.113902  1.208327   \n",
       "132746  0.068094 -0.216357  0.168529 -0.009598  1.463718  0.077280  1.200349   \n",
       "88076   0.113713 -0.105168  0.171326  0.046610  0.936521  0.018509  0.957564   \n",
       "\n",
       "              14        15        16        17        18        19        20  \\\n",
       "22570   0.135087 -0.701905  0.401269  0.770378 -0.384910  1.051266 -0.456784   \n",
       "16685   0.133532 -0.587394  0.306522  0.590236 -0.309718  0.814211 -0.454224   \n",
       "123440  0.097454 -0.642899  0.388071  0.740143 -0.353415  0.988833 -0.552584   \n",
       "132746  0.136644 -0.641120  0.443802  0.960702 -0.332269  1.157909 -0.418768   \n",
       "88076   0.142301 -0.556350  0.465833  0.813451 -0.281022  0.755103 -0.140946   \n",
       "\n",
       "              21        22        23        24        25        26        27  \\\n",
       "22570  -0.002182  0.541055  0.248358 -1.100184 -0.175203 -0.208046 -0.867965   \n",
       "16685  -0.044430  0.485063  0.254149 -0.977464 -0.069657 -0.162968 -0.706081   \n",
       "123440  0.008696  0.519598  0.220283 -1.153672 -0.086348 -0.296566 -0.891800   \n",
       "132746  0.007041  0.603946  0.343115 -0.831829 -0.259492 -0.152674 -0.754837   \n",
       "88076  -0.037548  0.373408  0.366028 -0.520116 -0.164377 -0.072388 -0.520891   \n",
       "\n",
       "              28        29        30        31        32        33        34  \\\n",
       "22570   0.499489  0.186703  0.184205  0.983467  0.171228 -0.473962 -0.067935   \n",
       "16685   0.385089  0.159863  0.103334  0.869885  0.189985 -0.437686 -0.103969   \n",
       "123440  0.575132  0.156740  0.151445  0.947815  0.176358 -0.513119 -0.141243   \n",
       "132746  0.562401  0.196445  0.208154  0.930183  0.174315 -0.362330 -0.174343   \n",
       "88076   0.369514  0.164744  0.123882  0.565505  0.194681 -0.176737 -0.141644   \n",
       "\n",
       "              35        36        37        38        39        40        41  \\\n",
       "22570  -0.746544  1.021289  1.183232  0.386953 -0.641643  0.311075 -0.683508   \n",
       "16685  -0.707402  0.890293  1.008038  0.294505 -0.596239  0.295053 -0.581890   \n",
       "123440 -0.855411  1.111049  1.181610  0.266485 -0.782169  0.305061 -0.756833   \n",
       "132746 -0.746526  1.024377  1.120853  0.412802 -0.643252  0.421478 -0.670171   \n",
       "88076  -0.483641  0.705575  0.762127  0.362083 -0.426424  0.307815 -0.444355   \n",
       "\n",
       "              42        43        44        45        46        47        48  \\\n",
       "22570   0.151159 -0.835884  0.314692  0.222327 -0.951957 -0.330503 -0.954284   \n",
       "16685   0.074221 -0.689581  0.240678  0.243086 -0.774149 -0.269381 -0.845396   \n",
       "123440  0.117234 -0.832389  0.225036  0.380719 -0.864756 -0.381530 -0.909695   \n",
       "132746  0.190803 -0.731712  0.375393  0.244755 -0.931101 -0.324493 -0.965189   \n",
       "88076   0.054236 -0.464765  0.433715  0.192827 -0.659232 -0.197318 -0.694278   \n",
       "\n",
       "              49        50        51        52        53        54        55  \\\n",
       "22570  -0.907088 -0.381297 -0.568039  0.165597  0.070025  0.762621  0.761588   \n",
       "16685  -0.747802 -0.357692 -0.575798  0.075748  0.053659  0.603290  0.588036   \n",
       "123440 -0.860609 -0.492003 -0.759491  0.094117  0.057793  0.702132  0.687047   \n",
       "132746 -0.913058 -0.375528 -0.599962  0.129909  0.137584  0.764300  0.757572   \n",
       "88076  -0.631891 -0.352255 -0.428348  0.049521  0.028710  0.419170  0.625482   \n",
       "\n",
       "              56        57        58        59        60        61        62  \\\n",
       "22570   0.041996 -0.491939  0.266267 -0.043106  0.338545 -0.468541 -0.267147   \n",
       "16685  -0.044867 -0.231689  0.222319 -0.028392  0.322678 -0.388349 -0.233056   \n",
       "123440 -0.006143 -0.398009  0.303515  0.107709  0.446104 -0.495050 -0.243958   \n",
       "132746  0.070390 -0.591878  0.294490 -0.020089  0.315416 -0.402238 -0.252954   \n",
       "88076   0.100326 -0.505123  0.227617 -0.046826  0.304113 -0.267223 -0.273584   \n",
       "\n",
       "              63        64        65        66        67        68        69  \\\n",
       "22570   0.514451 -0.990372  0.559909  0.069218  0.906935 -0.240570 -0.102345   \n",
       "16685   0.504491 -0.827719  0.423305  0.022377  0.745191 -0.158106 -0.051284   \n",
       "123440  0.582464 -1.070652  0.549145  0.059315  0.868515 -0.249779 -0.150307   \n",
       "132746  0.480173 -1.034404  0.558247  0.183675  0.872689 -0.360356 -0.175384   \n",
       "88076   0.283153 -0.827209  0.403577  0.145685  0.603928 -0.232856 -0.145204   \n",
       "\n",
       "              70        71        72        73        74        75        76  \\\n",
       "22570   0.305328  0.917522  1.085336 -0.495803  0.947908 -0.769751 -1.088021   \n",
       "16685   0.236679  0.785260  0.909442 -0.423034  0.779821 -0.655471 -0.895765   \n",
       "123440  0.288268  0.953198  1.076480 -0.557308  1.064692 -0.803946 -1.160680   \n",
       "132746  0.289037  0.867522  0.984221 -0.450662  1.052955 -0.635692 -1.088268   \n",
       "88076   0.172127  0.526596  0.688882 -0.250277  0.660606 -0.325578 -0.706362   \n",
       "\n",
       "              77        78        79        80        81        82        83  \\\n",
       "22570   0.469260  0.305072 -0.641360  0.150511  0.068960  1.043315 -0.418300   \n",
       "16685   0.324838  0.283415 -0.495799  0.173841  0.071632  0.870188 -0.370303   \n",
       "123440  0.444875  0.233066 -0.585349  0.186286  0.070056  1.118312 -0.422528   \n",
       "132746  0.407191  0.330033 -0.687119  0.054822 -0.060173  1.039292 -0.361207   \n",
       "88076   0.159460  0.271579 -0.558479  0.044891 -0.069888  0.651918 -0.311379   \n",
       "\n",
       "              84        85        86        87        88        89        90  \\\n",
       "22570   1.085144 -0.928959 -0.167872  0.812571 -0.183837 -0.667301  0.518967   \n",
       "16685   0.857812 -0.800465 -0.098202  0.706219 -0.139726 -0.653560  0.447379   \n",
       "123440  0.969558 -1.010211 -0.098995  0.764818 -0.184648 -0.706490  0.568750   \n",
       "132746  1.045623 -0.849056 -0.216978  0.812861 -0.111585 -0.619886  0.581188   \n",
       "88076   0.764495 -0.656329 -0.085217  0.496590 -0.094138 -0.396593  0.315139   \n",
       "\n",
       "              91        92        93        94        95        96        97  \\\n",
       "22570  -0.783982  0.553192  0.482868  0.470178  0.178276  0.539762  0.035636   \n",
       "16685  -0.664467  0.430281  0.468289  0.429375  0.127178  0.524980 -0.075853   \n",
       "123440 -0.762300  0.539530  0.464918  0.484420  0.236719  0.562145 -0.236458   \n",
       "132746 -0.719431  0.518276  0.367100  0.451986  0.147992  0.438494  0.064807   \n",
       "88076  -0.491023  0.301098  0.253831  0.315546  0.191494  0.312049 -0.006195   \n",
       "\n",
       "              98        99       100       101       102       103       104  \\\n",
       "22570  -0.467180  0.499896 -0.580248  0.625731  0.836909 -0.334144  0.471594   \n",
       "16685  -0.353152  0.462548 -0.459914  0.565993  0.650294 -0.299079  0.456827   \n",
       "123440 -0.464758  0.662213 -0.561075  0.655185  0.800641 -0.334941  0.421006   \n",
       "132746 -0.545258  0.530154 -0.565555  0.525659  0.894190 -0.343964  0.383234   \n",
       "88076  -0.323823  0.325932 -0.451024  0.278639  0.519498 -0.287530  0.275689   \n",
       "\n",
       "             105       106       107       108       109       110       111  \\\n",
       "22570   0.621171 -0.150757 -0.340110  0.281607 -0.465407 -0.528346  0.132581   \n",
       "16685   0.524460 -0.140988 -0.250238  0.290855 -0.331692 -0.456499  0.110501   \n",
       "123440  0.629312 -0.174035 -0.345552  0.307037 -0.442606 -0.579471  0.117530   \n",
       "132746  0.647721 -0.171504 -0.427961  0.227468 -0.462600 -0.574659  0.121805   \n",
       "88076   0.440547 -0.127253 -0.225619  0.151446 -0.322834 -0.445154  0.105608   \n",
       "\n",
       "             112       113       114       115       116       117       118  \\\n",
       "22570  -0.491218  0.739732  0.371598 -0.109128 -0.263545  0.003661  0.851574   \n",
       "16685  -0.394977  0.630778  0.327174 -0.140782 -0.256002  0.000502  0.696451   \n",
       "123440 -0.487585  0.723310  0.459936 -0.063990 -0.267776  0.000693  0.861442   \n",
       "132746 -0.529194  0.756697  0.366743 -0.023275 -0.280163  0.002904  0.942097   \n",
       "88076  -0.295885  0.657359  0.229650  0.060102 -0.212083 -0.069438  0.725006   \n",
       "\n",
       "             119       120       121       122       123       124       125  \\\n",
       "22570   0.607503  0.562765  0.457904  0.179819 -0.511334 -0.520777  0.024689   \n",
       "16685   0.415953  0.457629  0.395805  0.172674 -0.452176 -0.466752  0.021225   \n",
       "123440  0.550744  0.593329  0.406331  0.187094 -0.481162 -0.555893 -0.037544   \n",
       "132746  0.539076  0.560519  0.435520  0.161017 -0.549574 -0.412764 -0.055057   \n",
       "88076   0.269235  0.441460  0.249703  0.171428 -0.435611 -0.292266 -0.065743   \n",
       "\n",
       "             126       127  TARGET  \n",
       "22570  -0.189900 -0.254532     842  \n",
       "16685  -0.216684 -0.218758     355  \n",
       "123440 -0.190062 -0.358771     379  \n",
       "132746 -0.248691 -0.359678     991  \n",
       "88076  -0.147618 -0.175354       2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(os.path.join(output_dir, \"emb_train.csv\"), header=None)#.sample(frac=0.5)\n",
    "df_valid = pd.read_csv(os.path.join(output_dir, \"emb_valid.csv\"), header=None)\n",
    "print(\"train\", df_train.shape)\n",
    "print(\"valid\", df_valid.shape)\n",
    "df_train.columns = df_train.columns.tolist()[:-1] + [\"TARGET\"]\n",
    "df_valid.columns = df_valid.columns.tolist()[:-1] + [\"TARGET\"]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_train.columns.tolist()[:-1]\n",
    "n_classes = df_train[\"TARGET\"].nunique()\n",
    "labels_valid = df_valid[\"TARGET\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | learni... | max_depth | min_ch... | min_sp... | num_le... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[1]\tvalid_0's multi_logloss: 11.119\tvalid_1's multi_logloss: 3.23724\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's multi_logloss: 10.7781\tvalid_1's multi_logloss: 3.21482\n",
      "[3]\tvalid_0's multi_logloss: 10.4394\tvalid_1's multi_logloss: 3.19642\n",
      "[4]\tvalid_0's multi_logloss: 10.1033\tvalid_1's multi_logloss: 3.18325\n",
      "[5]\tvalid_0's multi_logloss: 9.77052\tvalid_1's multi_logloss: 3.17309\n",
      "[6]\tvalid_0's multi_logloss: 9.44253\tvalid_1's multi_logloss: 3.16966\n",
      "[7]\tvalid_0's multi_logloss: 9.12086\tvalid_1's multi_logloss: 3.17322\n",
      "[8]\tvalid_0's multi_logloss: 8.80827\tvalid_1's multi_logloss: 3.18861\n",
      "[9]\tvalid_0's multi_logloss: 8.50784\tvalid_1's multi_logloss: 3.21353\n",
      "[10]\tvalid_0's multi_logloss: 8.22358\tvalid_1's multi_logloss: 3.25602\n",
      "[11]\tvalid_0's multi_logloss: 7.96006\tvalid_1's multi_logloss: 3.31503\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's multi_logloss: 9.44253\tvalid_1's multi_logloss: 3.16966\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.124   \u001b[0m | \u001b[0m 0.9777  \u001b[0m | \u001b[0m 0.8065  \u001b[0m | \u001b[0m 559.3   \u001b[0m | \u001b[0m 123.4   \u001b[0m | \u001b[0m 0.6752  \u001b[0m | \u001b[0m 252.7   \u001b[0m | \u001b[0m 0.3528  \u001b[0m | \u001b[0m 0.1288  \u001b[0m | \u001b[0m 0.8574  \u001b[0m |\n",
      "[1]\tvalid_0's multi_logloss: 11.111\tvalid_1's multi_logloss: 3.2479\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's multi_logloss: 10.7951\tvalid_1's multi_logloss: 3.21925\n",
      "[3]\tvalid_0's multi_logloss: 10.4808\tvalid_1's multi_logloss: 3.19881\n",
      "[4]\tvalid_0's multi_logloss: 10.1687\tvalid_1's multi_logloss: 3.18201\n",
      "[5]\tvalid_0's multi_logloss: 9.85958\tvalid_1's multi_logloss: 3.1673\n",
      "[6]\tvalid_0's multi_logloss: 9.55356\tvalid_1's multi_logloss: 3.15737\n",
      "[7]\tvalid_0's multi_logloss: 9.25255\tvalid_1's multi_logloss: 3.15584\n",
      "[8]\tvalid_0's multi_logloss: 8.95823\tvalid_1's multi_logloss: 3.15965\n",
      "[9]\tvalid_0's multi_logloss: 8.67261\tvalid_1's multi_logloss: 3.17515\n",
      "[10]\tvalid_0's multi_logloss: 8.39864\tvalid_1's multi_logloss: 3.20142\n",
      "[11]\tvalid_0's multi_logloss: 8.1399\tvalid_1's multi_logloss: 3.24375\n",
      "[12]\tvalid_0's multi_logloss: 7.89943\tvalid_1's multi_logloss: 3.29694\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's multi_logloss: 9.25255\tvalid_1's multi_logloss: 3.15584\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.1129  \u001b[0m | \u001b[0m 0.9325  \u001b[0m | \u001b[0m 0.7443  \u001b[0m | \u001b[0m 779.1   \u001b[0m | \u001b[0m 122.3   \u001b[0m | \u001b[0m 0.4336  \u001b[0m | \u001b[0m 520.2   \u001b[0m | \u001b[0m 0.1061  \u001b[0m | \u001b[0m 0.2246  \u001b[0m | \u001b[0m 0.9006  \u001b[0m |\n",
      "[1]\tvalid_0's multi_logloss: 11.0972\tvalid_1's multi_logloss: 3.26419\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's multi_logloss: 10.8363\tvalid_1's multi_logloss: 3.24472\n",
      "[3]\tvalid_0's multi_logloss: 10.5767\tvalid_1's multi_logloss: 3.23109\n",
      "[4]\tvalid_0's multi_logloss: 10.3187\tvalid_1's multi_logloss: 3.21879\n",
      "[5]\tvalid_0's multi_logloss: 10.0621\tvalid_1's multi_logloss: 3.20577\n",
      "[6]\tvalid_0's multi_logloss: 9.80757\tvalid_1's multi_logloss: 3.19606\n",
      "[7]\tvalid_0's multi_logloss: 9.55593\tvalid_1's multi_logloss: 3.19239\n",
      "[8]\tvalid_0's multi_logloss: 9.30753\tvalid_1's multi_logloss: 3.19207\n",
      "[9]\tvalid_0's multi_logloss: 9.06362\tvalid_1's multi_logloss: 3.19582\n",
      "[10]\tvalid_0's multi_logloss: 8.82538\tvalid_1's multi_logloss: 3.20732\n",
      "[11]\tvalid_0's multi_logloss: 8.59405\tvalid_1's multi_logloss: 3.22772\n",
      "[12]\tvalid_0's multi_logloss: 8.37148\tvalid_1's multi_logloss: 3.25198\n",
      "[13]\tvalid_0's multi_logloss: 8.15981\tvalid_1's multi_logloss: 3.28711\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's multi_logloss: 9.30753\tvalid_1's multi_logloss: 3.19207\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.09879 \u001b[0m | \u001b[0m 0.9938  \u001b[0m | \u001b[0m 0.6178  \u001b[0m | \u001b[0m 105.6   \u001b[0m | \u001b[0m 118.9   \u001b[0m | \u001b[0m 0.8473  \u001b[0m | \u001b[0m 699.3   \u001b[0m | \u001b[0m 0.3606  \u001b[0m | \u001b[0m 0.1884  \u001b[0m | \u001b[0m 0.7898  \u001b[0m |\n",
      "[1]\tvalid_0's multi_logloss: 7.43894\tvalid_1's multi_logloss: 5.27064\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's multi_logloss: 7.27357\tvalid_1's multi_logloss: 5.1882\n"
     ]
    }
   ],
   "source": [
    "def accuracy(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    pred = np.argmax(preds.reshape(n_classes, len(preds)//n_classes), axis=0)\n",
    "    return 'accuracy', np.mean(labels == pred), True\n",
    "\n",
    "def lgbm_evaluate(**params):\n",
    "    start = time.time()\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "        \n",
    "    clf = LGBMClassifier(**params, \n",
    "                         n_estimators=10000, \n",
    "                         n_jobs=os.cpu_count(),\n",
    "                         objective=\"multiclass\",\n",
    "                         num_class=n_classes,\n",
    "                        )\n",
    "        \n",
    "    clf.fit(df_train[features].values, df_train[\"TARGET\"].values, \n",
    "            eval_set = [(df_train[features].values, df_train[\"TARGET\"].values),\n",
    "                        (df_valid[features].values, df_valid[\"TARGET\"].values)],\n",
    "            early_stopping_rounds=5, verbose=1)\n",
    "    \n",
    "#     train_preds = clf.predict_proba(df_train[features].values, num_iteration=clf.best_iteration_)\n",
    "    valid_preds = clf.predict_proba(df_valid[features].values, num_iteration=clf.best_iteration_)\n",
    "    \n",
    "#     print('Accuracy train {:.6f}'.format(sum(np.argmax(train_preds, axis=1) == df_train['TARGET'].values) / float(len(train_preds))))\n",
    "    acc_valid = np.mean(np.argmax(valid_preds, axis=1) == df_valid['TARGET'].values)\n",
    "        \n",
    "    return acc_valid\n",
    "\n",
    "def optimize_lgbm():\n",
    "    \n",
    "    params_space = {'colsample_bytree': (0.9, 1.0),\n",
    "                    'learning_rate': (0.01, 1.0), \n",
    "                    'num_leaves': (20, 1000), \n",
    "                    'subsample': (0.5, 1.0), \n",
    "                    'max_depth': (2, 1000), \n",
    "                    'reg_alpha': (0.0, 1.0), \n",
    "                    'reg_lambda': (0.0, 1.0), \n",
    "                    'min_split_gain': (0.0001, 1.),\n",
    "                    'min_child_weight': (5., 200.),\n",
    "                   }\n",
    "\n",
    "    bo = BayesianOptimization(lgbm_evaluate, params_space)\n",
    "    bo.maximize(init_points = 50, n_iter = 50)\n",
    "    \n",
    "    best_acc = bo.max['target']\n",
    "    best_params = bo.max['params']\n",
    "    best_params['num_leaves'] = int(best_params['num_leaves'])\n",
    "    best_params['max_depth'] = int(best_params['max_depth'])\n",
    "    \n",
    "    \n",
    "    print(\"Best validation acc: {}\".format(best_acc))\n",
    "    print('Best parameters found by optimization:\\n')\n",
    "    for k, v in best_params.items():\n",
    "        print(color.BLUE + k + color.END + ' = ' + color.BOLD + str(v)+ color.END + '     [',params_space[k],']')\n",
    "        \n",
    "    return best_acc, best_params\n",
    "\n",
    "best_acc, best_params = optimize_lgbm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------\n",
      "\n",
      "kfolded lightGBM\n",
      "\n",
      "Train set shape: (393420, 128)\n",
      "Valid set shape:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/var/python/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2016, 128)\n",
      "Number of features: 128\n",
      "[1]\ttraining's multi_logloss: 6.94317\ttraining's accuracy: 0.0142519\tvalid_1's multi_logloss: 5.68881\tvalid_1's accuracy: 0.121528\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's multi_logloss: 6.91078\ttraining's accuracy: 0.0209649\tvalid_1's multi_logloss: 5.65548\tvalid_1's accuracy: 0.167163\n",
      "[3]\ttraining's multi_logloss: 6.87927\ttraining's accuracy: 0.0262137\tvalid_1's multi_logloss: 5.62315\tvalid_1's accuracy: 0.176587\n",
      "[4]\ttraining's multi_logloss: 6.84904\ttraining's accuracy: 0.0296985\tvalid_1's multi_logloss: 5.59527\tvalid_1's accuracy: 0.186012\n",
      "[5]\ttraining's multi_logloss: 6.8195\ttraining's accuracy: 0.0320599\tvalid_1's multi_logloss: 5.56562\tvalid_1's accuracy: 0.189484\n",
      "[6]\ttraining's multi_logloss: 6.79073\ttraining's accuracy: 0.0333715\tvalid_1's multi_logloss: 5.53707\tvalid_1's accuracy: 0.194444\n",
      "[7]\ttraining's multi_logloss: 6.76224\ttraining's accuracy: 0.0348254\tvalid_1's multi_logloss: 5.50895\tvalid_1's accuracy: 0.201885\n",
      "[8]\ttraining's multi_logloss: 6.7347\ttraining's accuracy: 0.0357786\tvalid_1's multi_logloss: 5.48444\tvalid_1's accuracy: 0.203373\n",
      "[9]\ttraining's multi_logloss: 6.70776\ttraining's accuracy: 0.0366301\tvalid_1's multi_logloss: 5.45931\tvalid_1's accuracy: 0.204861\n",
      "[10]\ttraining's multi_logloss: 6.68123\ttraining's accuracy: 0.037113\tvalid_1's multi_logloss: 5.43642\tvalid_1's accuracy: 0.209821\n",
      "[11]\ttraining's multi_logloss: 6.6552\ttraining's accuracy: 0.037629\tvalid_1's multi_logloss: 5.41413\tvalid_1's accuracy: 0.214782\n",
      "[12]\ttraining's multi_logloss: 6.62953\ttraining's accuracy: 0.038328\tvalid_1's multi_logloss: 5.39234\tvalid_1's accuracy: 0.215774\n",
      "[13]\ttraining's multi_logloss: 6.60434\ttraining's accuracy: 0.0388008\tvalid_1's multi_logloss: 5.3735\tvalid_1's accuracy: 0.21379\n",
      "[14]\ttraining's multi_logloss: 6.57961\ttraining's accuracy: 0.0393218\tvalid_1's multi_logloss: 5.35451\tvalid_1's accuracy: 0.219246\n",
      "[15]\ttraining's multi_logloss: 6.55526\ttraining's accuracy: 0.039909\tvalid_1's multi_logloss: 5.33649\tvalid_1's accuracy: 0.220238\n",
      "[16]\ttraining's multi_logloss: 6.53122\ttraining's accuracy: 0.0402801\tvalid_1's multi_logloss: 5.31793\tvalid_1's accuracy: 0.22123\n",
      "[17]\ttraining's multi_logloss: 6.50762\ttraining's accuracy: 0.0407554\tvalid_1's multi_logloss: 5.30143\tvalid_1's accuracy: 0.219246\n",
      "[18]\ttraining's multi_logloss: 6.48436\ttraining's accuracy: 0.0412562\tvalid_1's multi_logloss: 5.28421\tvalid_1's accuracy: 0.220238\n",
      "[19]\ttraining's multi_logloss: 6.46145\ttraining's accuracy: 0.041701\tvalid_1's multi_logloss: 5.26847\tvalid_1's accuracy: 0.220734\n",
      "[20]\ttraining's multi_logloss: 6.43913\ttraining's accuracy: 0.0422398\tvalid_1's multi_logloss: 5.25294\tvalid_1's accuracy: 0.222222\n",
      "[21]\ttraining's multi_logloss: 6.41684\ttraining's accuracy: 0.0428524\tvalid_1's multi_logloss: 5.23901\tvalid_1's accuracy: 0.22123\n",
      "[22]\ttraining's multi_logloss: 6.39484\ttraining's accuracy: 0.0433684\tvalid_1's multi_logloss: 5.22462\tvalid_1's accuracy: 0.222718\n",
      "[23]\ttraining's multi_logloss: 6.37337\ttraining's accuracy: 0.0439429\tvalid_1's multi_logloss: 5.21079\tvalid_1's accuracy: 0.223214\n",
      "[24]\ttraining's multi_logloss: 6.35216\ttraining's accuracy: 0.0445732\tvalid_1's multi_logloss: 5.1981\tvalid_1's accuracy: 0.223214\n",
      "[25]\ttraining's multi_logloss: 6.33121\ttraining's accuracy: 0.0451451\tvalid_1's multi_logloss: 5.18471\tvalid_1's accuracy: 0.226687\n",
      "[26]\ttraining's multi_logloss: 6.31064\ttraining's accuracy: 0.0456967\tvalid_1's multi_logloss: 5.17244\tvalid_1's accuracy: 0.224702\n",
      "[27]\ttraining's multi_logloss: 6.29016\ttraining's accuracy: 0.0462991\tvalid_1's multi_logloss: 5.16061\tvalid_1's accuracy: 0.227183\n",
      "[28]\ttraining's multi_logloss: 6.27008\ttraining's accuracy: 0.0469015\tvalid_1's multi_logloss: 5.14907\tvalid_1's accuracy: 0.227183\n",
      "[29]\ttraining's multi_logloss: 6.25025\ttraining's accuracy: 0.0475065\tvalid_1's multi_logloss: 5.13851\tvalid_1's accuracy: 0.228671\n",
      "[30]\ttraining's multi_logloss: 6.23055\ttraining's accuracy: 0.0481165\tvalid_1's multi_logloss: 5.12713\tvalid_1's accuracy: 0.226687\n",
      "[31]\ttraining's multi_logloss: 6.21119\ttraining's accuracy: 0.0487647\tvalid_1's multi_logloss: 5.11736\tvalid_1's accuracy: 0.229167\n",
      "[32]\ttraining's multi_logloss: 6.19201\ttraining's accuracy: 0.0495018\tvalid_1's multi_logloss: 5.10722\tvalid_1's accuracy: 0.226687\n",
      "[33]\ttraining's multi_logloss: 6.17308\ttraining's accuracy: 0.0502008\tvalid_1's multi_logloss: 5.09741\tvalid_1's accuracy: 0.22371\n",
      "[34]\ttraining's multi_logloss: 6.15452\ttraining's accuracy: 0.0507702\tvalid_1's multi_logloss: 5.08784\tvalid_1's accuracy: 0.221726\n",
      "[35]\ttraining's multi_logloss: 6.13603\ttraining's accuracy: 0.0516496\tvalid_1's multi_logloss: 5.07997\tvalid_1's accuracy: 0.221726\n",
      "[36]\ttraining's multi_logloss: 6.11767\ttraining's accuracy: 0.052369\tvalid_1's multi_logloss: 5.07169\tvalid_1's accuracy: 0.218254\n",
      "[37]\ttraining's multi_logloss: 6.09957\ttraining's accuracy: 0.0532662\tvalid_1's multi_logloss: 5.06333\tvalid_1's accuracy: 0.215774\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-e87d9f737f9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m }\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlightgbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-e87d9f737f9f>\u001b[0m in \u001b[0;36mlightgbm\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'...model trained'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/var/python/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid_sets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m             \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/var/python/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36meval_train\u001b[0;34m(self, feval)\u001b[0m\n\u001b[1;32m   1956\u001b[0m             \u001b[0mList\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \"\"\"\n\u001b[0;32m-> 1958\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__inner_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__train_data_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/var/python/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__inner_eval\u001b[0;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[1;32m   2362\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2363\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_out_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2364\u001b[0;31m                 result.ctypes.data_as(ctypes.POINTER(ctypes.c_double))))\n\u001b[0m\u001b[1;32m   2365\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtmp_out_len\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_inner_eval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2366\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wrong length of eval results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "def lightgbm(params):\n",
    "    \n",
    "    print('\\n--------------------------------------------\\n')\n",
    "    print('kfolded lightGBM\\n')\n",
    "    \n",
    "    print('Train set shape:', dtrain.data.shape)\n",
    "    print('Valid set shape:', dvalid.data.shape)\n",
    "\n",
    "#     # Create arrays and dataframes to store results\n",
    "#     oof_preds = np.zeros(df_train.shape[0])\n",
    "#     sub_preds = np.zeros(df_test.shape[0])\n",
    "#     df_feature_importance = pd.DataFrame()\n",
    "    \n",
    "    print('Number of features: {}'.format(len(features)))\n",
    "                    \n",
    "    def accuracy(preds, train_data):\n",
    "        labels = train_data.get_label()\n",
    "        pred = np.argmax(preds.reshape(n_classes, len(preds)//n_classes), axis=0)\n",
    "        return 'accuracy', np.mean(labels == pred), True\n",
    "    \n",
    "    clf = lgb.train(\n",
    "        params=params,\n",
    "        train_set=dtrain,\n",
    "#         num_boost_round=10000,\n",
    "        valid_sets=[dtrain, dvalid],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=True,\n",
    "        feval=accuracy,\n",
    "    )\n",
    "    print('...model trained')\n",
    "\n",
    "    train_preds = clf.predict(dtrain.data)\n",
    "    valid_preds = clf.predict(dvalid.data)\n",
    "    print('...predictions made')\n",
    "    print('Accuracy train {:.6f}'.format(np.mean(np.argmax(train_preds, axis=1) == df_train['TARGET'].values)) )\n",
    "    print('Accuracy valid {:.6f}'.format(np.mean(np.argmax(valid_preds, axis=1) == df_valid['TARGET'].values)) )\n",
    "    return clf\n",
    "\n",
    "params = {  \"objective\" : \"multiclass\",\n",
    "            \"num_class\" : n_classes,\n",
    "            'n_estimators': 10000,\n",
    "            'learning_rate': .02,\n",
    "            'num_leaves': 1000,\n",
    "            'colsample_bytree': 1.,\n",
    "            'subsample': 1.,\n",
    "            'max_depth': 100,\n",
    "            'reg_alpha': .041545473,\n",
    "            'reg_lambda': .0735294,\n",
    "            'min_split_gain': .0222415,\n",
    "            'min_child_weight': 39.3259775,                \n",
    "#             \"device_type\" : \"gpu\",\n",
    "            \"njobs\" : os.cpu_count(),\n",
    "}\n",
    "\n",
    "clf = lightgbm(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = clf.predict(dtrain.data)\n",
    "valid_preds = clf.predict(dvalid.data)\n",
    "train_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39342,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(train_preds, axis=1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
