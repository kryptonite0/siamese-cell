{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightGBM version 2.2.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from bayes_opt import BayesianOptimization\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import settings_model\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_rows', 50000)\n",
    "pd.set_option('display.max_columns', 50000)\n",
    "\n",
    "print('lightGBM version', lgb.__version__)\n",
    "\n",
    "# model_id = \"HUVEC_20190810_202545\"\n",
    "model_id = \"HEPG2_20190811_142600\"\n",
    "output_dir = os.path.join(settings_model.root_path, \"models\", \"siamese-cell\",\n",
    "                          f\"{model_id}\", \"emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (86220, 129)\n",
      "valid (992, 129)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81684</th>\n",
       "      <td>1.452379</td>\n",
       "      <td>-1.002719</td>\n",
       "      <td>1.369243</td>\n",
       "      <td>0.701099</td>\n",
       "      <td>-0.009637</td>\n",
       "      <td>0.500139</td>\n",
       "      <td>0.496532</td>\n",
       "      <td>0.047666</td>\n",
       "      <td>-0.210582</td>\n",
       "      <td>0.140533</td>\n",
       "      <td>-0.276183</td>\n",
       "      <td>1.958663</td>\n",
       "      <td>0.165886</td>\n",
       "      <td>1.583723</td>\n",
       "      <td>0.166616</td>\n",
       "      <td>-0.836239</td>\n",
       "      <td>0.514721</td>\n",
       "      <td>0.961807</td>\n",
       "      <td>-0.471568</td>\n",
       "      <td>1.212610</td>\n",
       "      <td>-0.763469</td>\n",
       "      <td>-0.078843</td>\n",
       "      <td>0.736094</td>\n",
       "      <td>0.379265</td>\n",
       "      <td>-1.554926</td>\n",
       "      <td>-0.072742</td>\n",
       "      <td>-0.238671</td>\n",
       "      <td>-1.151327</td>\n",
       "      <td>0.656669</td>\n",
       "      <td>0.281587</td>\n",
       "      <td>0.116646</td>\n",
       "      <td>1.365107</td>\n",
       "      <td>0.310523</td>\n",
       "      <td>-0.693532</td>\n",
       "      <td>-0.233098</td>\n",
       "      <td>-1.133831</td>\n",
       "      <td>1.514880</td>\n",
       "      <td>1.557180</td>\n",
       "      <td>0.439971</td>\n",
       "      <td>-0.984416</td>\n",
       "      <td>0.313255</td>\n",
       "      <td>-1.054495</td>\n",
       "      <td>0.065787</td>\n",
       "      <td>-1.041578</td>\n",
       "      <td>0.363855</td>\n",
       "      <td>0.437784</td>\n",
       "      <td>-1.192662</td>\n",
       "      <td>-0.571464</td>\n",
       "      <td>-1.302940</td>\n",
       "      <td>-1.093599</td>\n",
       "      <td>-0.601988</td>\n",
       "      <td>-0.941053</td>\n",
       "      <td>0.137043</td>\n",
       "      <td>0.144300</td>\n",
       "      <td>0.932341</td>\n",
       "      <td>0.910831</td>\n",
       "      <td>-0.089672</td>\n",
       "      <td>-0.400760</td>\n",
       "      <td>0.397046</td>\n",
       "      <td>0.074110</td>\n",
       "      <td>0.621118</td>\n",
       "      <td>-0.671983</td>\n",
       "      <td>-0.392577</td>\n",
       "      <td>0.788927</td>\n",
       "      <td>-1.376851</td>\n",
       "      <td>0.628460</td>\n",
       "      <td>0.051426</td>\n",
       "      <td>1.232759</td>\n",
       "      <td>-0.262301</td>\n",
       "      <td>-0.109760</td>\n",
       "      <td>0.451482</td>\n",
       "      <td>1.293782</td>\n",
       "      <td>1.526669</td>\n",
       "      <td>-0.739702</td>\n",
       "      <td>1.279637</td>\n",
       "      <td>-0.954438</td>\n",
       "      <td>-1.503910</td>\n",
       "      <td>0.547250</td>\n",
       "      <td>0.389500</td>\n",
       "      <td>-0.819997</td>\n",
       "      <td>0.309607</td>\n",
       "      <td>0.136138</td>\n",
       "      <td>1.451662</td>\n",
       "      <td>-0.515017</td>\n",
       "      <td>1.303103</td>\n",
       "      <td>-1.265178</td>\n",
       "      <td>-0.071492</td>\n",
       "      <td>1.104644</td>\n",
       "      <td>-0.209296</td>\n",
       "      <td>-1.004866</td>\n",
       "      <td>0.687148</td>\n",
       "      <td>-0.979767</td>\n",
       "      <td>0.651393</td>\n",
       "      <td>0.722044</td>\n",
       "      <td>0.673126</td>\n",
       "      <td>0.200752</td>\n",
       "      <td>0.862134</td>\n",
       "      <td>-0.239570</td>\n",
       "      <td>-0.529020</td>\n",
       "      <td>0.847729</td>\n",
       "      <td>-0.693538</td>\n",
       "      <td>0.878455</td>\n",
       "      <td>1.047674</td>\n",
       "      <td>-0.483992</td>\n",
       "      <td>0.560848</td>\n",
       "      <td>0.893744</td>\n",
       "      <td>-0.238188</td>\n",
       "      <td>-0.399435</td>\n",
       "      <td>0.489400</td>\n",
       "      <td>-0.593171</td>\n",
       "      <td>-0.778336</td>\n",
       "      <td>0.281279</td>\n",
       "      <td>-0.629935</td>\n",
       "      <td>0.955278</td>\n",
       "      <td>0.598631</td>\n",
       "      <td>-0.127998</td>\n",
       "      <td>-0.443079</td>\n",
       "      <td>-0.050284</td>\n",
       "      <td>1.093501</td>\n",
       "      <td>0.555158</td>\n",
       "      <td>0.729942</td>\n",
       "      <td>0.664189</td>\n",
       "      <td>0.270384</td>\n",
       "      <td>-0.758777</td>\n",
       "      <td>-0.777352</td>\n",
       "      <td>0.108012</td>\n",
       "      <td>-0.311805</td>\n",
       "      <td>-0.354702</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157911</th>\n",
       "      <td>1.486251</td>\n",
       "      <td>-0.967420</td>\n",
       "      <td>1.536840</td>\n",
       "      <td>0.637121</td>\n",
       "      <td>-0.085225</td>\n",
       "      <td>0.618029</td>\n",
       "      <td>0.512894</td>\n",
       "      <td>0.187284</td>\n",
       "      <td>-0.282539</td>\n",
       "      <td>0.121434</td>\n",
       "      <td>-0.292126</td>\n",
       "      <td>1.878531</td>\n",
       "      <td>-0.079271</td>\n",
       "      <td>1.632847</td>\n",
       "      <td>0.122785</td>\n",
       "      <td>-0.999917</td>\n",
       "      <td>0.535181</td>\n",
       "      <td>0.820863</td>\n",
       "      <td>-0.416541</td>\n",
       "      <td>1.166093</td>\n",
       "      <td>-0.848064</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>0.632828</td>\n",
       "      <td>0.377507</td>\n",
       "      <td>-1.843385</td>\n",
       "      <td>-0.022893</td>\n",
       "      <td>-0.185876</td>\n",
       "      <td>-1.058074</td>\n",
       "      <td>0.740442</td>\n",
       "      <td>0.227765</td>\n",
       "      <td>-0.049946</td>\n",
       "      <td>1.566132</td>\n",
       "      <td>0.183009</td>\n",
       "      <td>-0.792614</td>\n",
       "      <td>-0.269539</td>\n",
       "      <td>-1.121393</td>\n",
       "      <td>1.605000</td>\n",
       "      <td>1.561734</td>\n",
       "      <td>0.501031</td>\n",
       "      <td>-1.088469</td>\n",
       "      <td>0.518020</td>\n",
       "      <td>-0.770219</td>\n",
       "      <td>-0.042091</td>\n",
       "      <td>-0.932183</td>\n",
       "      <td>0.285804</td>\n",
       "      <td>0.455504</td>\n",
       "      <td>-1.144219</td>\n",
       "      <td>-0.475844</td>\n",
       "      <td>-1.203283</td>\n",
       "      <td>-1.177307</td>\n",
       "      <td>-0.642624</td>\n",
       "      <td>-0.950677</td>\n",
       "      <td>0.118128</td>\n",
       "      <td>0.020213</td>\n",
       "      <td>0.876312</td>\n",
       "      <td>1.092227</td>\n",
       "      <td>0.013274</td>\n",
       "      <td>-0.335458</td>\n",
       "      <td>0.444384</td>\n",
       "      <td>-0.040574</td>\n",
       "      <td>0.669704</td>\n",
       "      <td>-0.594840</td>\n",
       "      <td>-0.161697</td>\n",
       "      <td>0.816085</td>\n",
       "      <td>-1.427242</td>\n",
       "      <td>0.465762</td>\n",
       "      <td>-0.071908</td>\n",
       "      <td>1.336919</td>\n",
       "      <td>-0.481902</td>\n",
       "      <td>-0.183875</td>\n",
       "      <td>0.553800</td>\n",
       "      <td>1.406928</td>\n",
       "      <td>1.468062</td>\n",
       "      <td>-0.789349</td>\n",
       "      <td>1.082343</td>\n",
       "      <td>-1.148844</td>\n",
       "      <td>-1.371884</td>\n",
       "      <td>0.501040</td>\n",
       "      <td>0.565724</td>\n",
       "      <td>-0.782709</td>\n",
       "      <td>0.299454</td>\n",
       "      <td>0.212317</td>\n",
       "      <td>1.365652</td>\n",
       "      <td>-0.572387</td>\n",
       "      <td>1.318753</td>\n",
       "      <td>-1.526228</td>\n",
       "      <td>0.106109</td>\n",
       "      <td>0.931194</td>\n",
       "      <td>-0.150863</td>\n",
       "      <td>-0.925325</td>\n",
       "      <td>0.741164</td>\n",
       "      <td>-0.925167</td>\n",
       "      <td>0.455999</td>\n",
       "      <td>0.938158</td>\n",
       "      <td>0.687450</td>\n",
       "      <td>0.229408</td>\n",
       "      <td>0.893249</td>\n",
       "      <td>-0.257193</td>\n",
       "      <td>-0.385989</td>\n",
       "      <td>0.812355</td>\n",
       "      <td>-0.798476</td>\n",
       "      <td>0.873031</td>\n",
       "      <td>0.889743</td>\n",
       "      <td>-0.514877</td>\n",
       "      <td>0.657277</td>\n",
       "      <td>1.003032</td>\n",
       "      <td>-0.246083</td>\n",
       "      <td>-0.497816</td>\n",
       "      <td>0.403705</td>\n",
       "      <td>-0.359008</td>\n",
       "      <td>-0.929250</td>\n",
       "      <td>0.235381</td>\n",
       "      <td>-0.533468</td>\n",
       "      <td>1.035063</td>\n",
       "      <td>0.751790</td>\n",
       "      <td>-0.218244</td>\n",
       "      <td>-0.332239</td>\n",
       "      <td>-0.144424</td>\n",
       "      <td>1.094039</td>\n",
       "      <td>0.634521</td>\n",
       "      <td>0.907201</td>\n",
       "      <td>0.700801</td>\n",
       "      <td>0.408076</td>\n",
       "      <td>-0.738221</td>\n",
       "      <td>-1.055755</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>-0.518598</td>\n",
       "      <td>-0.114026</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80826</th>\n",
       "      <td>0.803646</td>\n",
       "      <td>-0.488392</td>\n",
       "      <td>0.661941</td>\n",
       "      <td>0.344817</td>\n",
       "      <td>-0.123173</td>\n",
       "      <td>0.282972</td>\n",
       "      <td>0.167541</td>\n",
       "      <td>0.067699</td>\n",
       "      <td>-0.077539</td>\n",
       "      <td>0.079486</td>\n",
       "      <td>-0.049094</td>\n",
       "      <td>1.067595</td>\n",
       "      <td>0.090799</td>\n",
       "      <td>0.882457</td>\n",
       "      <td>0.137043</td>\n",
       "      <td>-0.496857</td>\n",
       "      <td>0.337070</td>\n",
       "      <td>0.641052</td>\n",
       "      <td>-0.292046</td>\n",
       "      <td>0.811200</td>\n",
       "      <td>-0.236791</td>\n",
       "      <td>0.003213</td>\n",
       "      <td>0.393449</td>\n",
       "      <td>0.225089</td>\n",
       "      <td>-0.667992</td>\n",
       "      <td>-0.178938</td>\n",
       "      <td>-0.119999</td>\n",
       "      <td>-0.575368</td>\n",
       "      <td>0.351424</td>\n",
       "      <td>0.131639</td>\n",
       "      <td>0.148536</td>\n",
       "      <td>0.624327</td>\n",
       "      <td>0.154565</td>\n",
       "      <td>-0.262047</td>\n",
       "      <td>-0.109486</td>\n",
       "      <td>-0.540456</td>\n",
       "      <td>0.718081</td>\n",
       "      <td>0.769104</td>\n",
       "      <td>0.293621</td>\n",
       "      <td>-0.456438</td>\n",
       "      <td>0.270199</td>\n",
       "      <td>-0.500269</td>\n",
       "      <td>0.113389</td>\n",
       "      <td>-0.557005</td>\n",
       "      <td>0.321575</td>\n",
       "      <td>0.222165</td>\n",
       "      <td>-0.693677</td>\n",
       "      <td>-0.222910</td>\n",
       "      <td>-0.692264</td>\n",
       "      <td>-0.647913</td>\n",
       "      <td>-0.317627</td>\n",
       "      <td>-0.426342</td>\n",
       "      <td>0.084019</td>\n",
       "      <td>0.029069</td>\n",
       "      <td>0.485115</td>\n",
       "      <td>0.560512</td>\n",
       "      <td>0.034003</td>\n",
       "      <td>-0.427284</td>\n",
       "      <td>0.229681</td>\n",
       "      <td>-0.011451</td>\n",
       "      <td>0.251503</td>\n",
       "      <td>-0.282460</td>\n",
       "      <td>-0.191643</td>\n",
       "      <td>0.315873</td>\n",
       "      <td>-0.789709</td>\n",
       "      <td>0.421840</td>\n",
       "      <td>0.089766</td>\n",
       "      <td>0.596190</td>\n",
       "      <td>-0.186518</td>\n",
       "      <td>-0.073099</td>\n",
       "      <td>0.161928</td>\n",
       "      <td>0.556822</td>\n",
       "      <td>0.702570</td>\n",
       "      <td>-0.321393</td>\n",
       "      <td>0.768069</td>\n",
       "      <td>-0.469178</td>\n",
       "      <td>-0.743509</td>\n",
       "      <td>0.291852</td>\n",
       "      <td>0.237719</td>\n",
       "      <td>-0.489482</td>\n",
       "      <td>0.064605</td>\n",
       "      <td>-0.026531</td>\n",
       "      <td>0.735572</td>\n",
       "      <td>-0.261231</td>\n",
       "      <td>0.779453</td>\n",
       "      <td>-0.653346</td>\n",
       "      <td>-0.110785</td>\n",
       "      <td>0.519304</td>\n",
       "      <td>-0.089215</td>\n",
       "      <td>-0.430857</td>\n",
       "      <td>0.354168</td>\n",
       "      <td>-0.535983</td>\n",
       "      <td>0.404685</td>\n",
       "      <td>0.249762</td>\n",
       "      <td>0.344466</td>\n",
       "      <td>0.179717</td>\n",
       "      <td>0.351801</td>\n",
       "      <td>-0.038659</td>\n",
       "      <td>-0.317754</td>\n",
       "      <td>0.359617</td>\n",
       "      <td>-0.409534</td>\n",
       "      <td>0.369182</td>\n",
       "      <td>0.594447</td>\n",
       "      <td>-0.228245</td>\n",
       "      <td>0.258842</td>\n",
       "      <td>0.449706</td>\n",
       "      <td>-0.103915</td>\n",
       "      <td>-0.230782</td>\n",
       "      <td>0.167989</td>\n",
       "      <td>-0.340933</td>\n",
       "      <td>-0.405068</td>\n",
       "      <td>0.055126</td>\n",
       "      <td>-0.334929</td>\n",
       "      <td>0.560386</td>\n",
       "      <td>0.231980</td>\n",
       "      <td>-0.033686</td>\n",
       "      <td>-0.179390</td>\n",
       "      <td>0.002565</td>\n",
       "      <td>0.673102</td>\n",
       "      <td>0.371692</td>\n",
       "      <td>0.450679</td>\n",
       "      <td>0.268195</td>\n",
       "      <td>0.135310</td>\n",
       "      <td>-0.367949</td>\n",
       "      <td>-0.316786</td>\n",
       "      <td>-0.049914</td>\n",
       "      <td>-0.076540</td>\n",
       "      <td>-0.202827</td>\n",
       "      <td>913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20015</th>\n",
       "      <td>1.379577</td>\n",
       "      <td>-1.019494</td>\n",
       "      <td>1.305764</td>\n",
       "      <td>0.618841</td>\n",
       "      <td>-0.104208</td>\n",
       "      <td>0.564617</td>\n",
       "      <td>0.473198</td>\n",
       "      <td>0.031544</td>\n",
       "      <td>-0.245656</td>\n",
       "      <td>0.225477</td>\n",
       "      <td>-0.219041</td>\n",
       "      <td>1.846485</td>\n",
       "      <td>0.052613</td>\n",
       "      <td>1.435924</td>\n",
       "      <td>0.116540</td>\n",
       "      <td>-0.822238</td>\n",
       "      <td>0.438163</td>\n",
       "      <td>0.861800</td>\n",
       "      <td>-0.451325</td>\n",
       "      <td>1.268867</td>\n",
       "      <td>-0.604564</td>\n",
       "      <td>0.083039</td>\n",
       "      <td>0.622512</td>\n",
       "      <td>0.211039</td>\n",
       "      <td>-1.334945</td>\n",
       "      <td>-0.199346</td>\n",
       "      <td>-0.310774</td>\n",
       "      <td>-1.015583</td>\n",
       "      <td>0.679428</td>\n",
       "      <td>0.171515</td>\n",
       "      <td>0.252509</td>\n",
       "      <td>1.138689</td>\n",
       "      <td>0.168829</td>\n",
       "      <td>-0.588593</td>\n",
       "      <td>-0.119247</td>\n",
       "      <td>-0.956236</td>\n",
       "      <td>1.285724</td>\n",
       "      <td>1.466012</td>\n",
       "      <td>0.428442</td>\n",
       "      <td>-0.840155</td>\n",
       "      <td>0.377839</td>\n",
       "      <td>-0.834500</td>\n",
       "      <td>0.170125</td>\n",
       "      <td>-0.991207</td>\n",
       "      <td>0.271994</td>\n",
       "      <td>0.288017</td>\n",
       "      <td>-1.048971</td>\n",
       "      <td>-0.446393</td>\n",
       "      <td>-1.120854</td>\n",
       "      <td>-1.094619</td>\n",
       "      <td>-0.496792</td>\n",
       "      <td>-0.786371</td>\n",
       "      <td>0.149387</td>\n",
       "      <td>0.139309</td>\n",
       "      <td>0.981824</td>\n",
       "      <td>0.879634</td>\n",
       "      <td>0.020316</td>\n",
       "      <td>-0.535410</td>\n",
       "      <td>0.326228</td>\n",
       "      <td>0.047893</td>\n",
       "      <td>0.383449</td>\n",
       "      <td>-0.653878</td>\n",
       "      <td>-0.252492</td>\n",
       "      <td>0.723517</td>\n",
       "      <td>-1.177697</td>\n",
       "      <td>0.655013</td>\n",
       "      <td>0.065887</td>\n",
       "      <td>1.097127</td>\n",
       "      <td>-0.318590</td>\n",
       "      <td>-0.144530</td>\n",
       "      <td>0.351692</td>\n",
       "      <td>1.148292</td>\n",
       "      <td>1.315205</td>\n",
       "      <td>-0.691845</td>\n",
       "      <td>1.205920</td>\n",
       "      <td>-1.002308</td>\n",
       "      <td>-1.385365</td>\n",
       "      <td>0.516487</td>\n",
       "      <td>0.297656</td>\n",
       "      <td>-0.711724</td>\n",
       "      <td>0.227522</td>\n",
       "      <td>0.093147</td>\n",
       "      <td>1.306324</td>\n",
       "      <td>-0.535595</td>\n",
       "      <td>1.261416</td>\n",
       "      <td>-1.122192</td>\n",
       "      <td>-0.142971</td>\n",
       "      <td>0.971361</td>\n",
       "      <td>-0.230418</td>\n",
       "      <td>-0.899607</td>\n",
       "      <td>0.666209</td>\n",
       "      <td>-0.949601</td>\n",
       "      <td>0.673725</td>\n",
       "      <td>0.603956</td>\n",
       "      <td>0.534082</td>\n",
       "      <td>0.169462</td>\n",
       "      <td>0.608821</td>\n",
       "      <td>-0.065431</td>\n",
       "      <td>-0.614474</td>\n",
       "      <td>0.762683</td>\n",
       "      <td>-0.639805</td>\n",
       "      <td>0.844612</td>\n",
       "      <td>1.073392</td>\n",
       "      <td>-0.363619</td>\n",
       "      <td>0.610556</td>\n",
       "      <td>0.759537</td>\n",
       "      <td>-0.140858</td>\n",
       "      <td>-0.438278</td>\n",
       "      <td>0.367778</td>\n",
       "      <td>-0.602195</td>\n",
       "      <td>-0.643786</td>\n",
       "      <td>0.116179</td>\n",
       "      <td>-0.678912</td>\n",
       "      <td>0.837203</td>\n",
       "      <td>0.478550</td>\n",
       "      <td>-0.078266</td>\n",
       "      <td>-0.311366</td>\n",
       "      <td>0.018070</td>\n",
       "      <td>1.063512</td>\n",
       "      <td>0.731462</td>\n",
       "      <td>0.656659</td>\n",
       "      <td>0.547255</td>\n",
       "      <td>0.151091</td>\n",
       "      <td>-0.598321</td>\n",
       "      <td>-0.653324</td>\n",
       "      <td>-0.005014</td>\n",
       "      <td>-0.338659</td>\n",
       "      <td>-0.403436</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122937</th>\n",
       "      <td>10.063936</td>\n",
       "      <td>-4.303458</td>\n",
       "      <td>9.384829</td>\n",
       "      <td>3.504562</td>\n",
       "      <td>0.041939</td>\n",
       "      <td>2.237945</td>\n",
       "      <td>3.282542</td>\n",
       "      <td>2.435402</td>\n",
       "      <td>-1.531437</td>\n",
       "      <td>-0.442141</td>\n",
       "      <td>-2.631069</td>\n",
       "      <td>10.499796</td>\n",
       "      <td>0.756974</td>\n",
       "      <td>10.981818</td>\n",
       "      <td>0.474040</td>\n",
       "      <td>-8.082710</td>\n",
       "      <td>2.143572</td>\n",
       "      <td>4.524682</td>\n",
       "      <td>-2.882796</td>\n",
       "      <td>5.800526</td>\n",
       "      <td>-7.650096</td>\n",
       "      <td>-1.994673</td>\n",
       "      <td>6.436169</td>\n",
       "      <td>4.224161</td>\n",
       "      <td>-13.470854</td>\n",
       "      <td>0.714606</td>\n",
       "      <td>-0.428849</td>\n",
       "      <td>-4.408454</td>\n",
       "      <td>4.168980</td>\n",
       "      <td>1.539393</td>\n",
       "      <td>-1.285654</td>\n",
       "      <td>13.198402</td>\n",
       "      <td>1.674970</td>\n",
       "      <td>-5.881006</td>\n",
       "      <td>-1.104560</td>\n",
       "      <td>-8.885115</td>\n",
       "      <td>11.447929</td>\n",
       "      <td>9.250537</td>\n",
       "      <td>4.232334</td>\n",
       "      <td>-6.715809</td>\n",
       "      <td>2.645887</td>\n",
       "      <td>-5.093472</td>\n",
       "      <td>-0.301643</td>\n",
       "      <td>-5.834190</td>\n",
       "      <td>0.130726</td>\n",
       "      <td>5.268610</td>\n",
       "      <td>-6.808812</td>\n",
       "      <td>-2.546334</td>\n",
       "      <td>-7.562334</td>\n",
       "      <td>-8.913692</td>\n",
       "      <td>-4.610030</td>\n",
       "      <td>-6.939877</td>\n",
       "      <td>0.590639</td>\n",
       "      <td>2.077278</td>\n",
       "      <td>3.614421</td>\n",
       "      <td>7.006286</td>\n",
       "      <td>0.051536</td>\n",
       "      <td>-0.961448</td>\n",
       "      <td>2.701857</td>\n",
       "      <td>-1.155196</td>\n",
       "      <td>5.638363</td>\n",
       "      <td>-2.273206</td>\n",
       "      <td>-0.108356</td>\n",
       "      <td>6.335777</td>\n",
       "      <td>-10.119603</td>\n",
       "      <td>3.371395</td>\n",
       "      <td>-1.579070</td>\n",
       "      <td>7.743876</td>\n",
       "      <td>-2.859392</td>\n",
       "      <td>-0.832090</td>\n",
       "      <td>4.649530</td>\n",
       "      <td>9.663132</td>\n",
       "      <td>8.414061</td>\n",
       "      <td>-4.617828</td>\n",
       "      <td>6.558440</td>\n",
       "      <td>-7.892317</td>\n",
       "      <td>-9.259053</td>\n",
       "      <td>2.537635</td>\n",
       "      <td>4.648840</td>\n",
       "      <td>-4.541659</td>\n",
       "      <td>2.142683</td>\n",
       "      <td>1.719665</td>\n",
       "      <td>6.481392</td>\n",
       "      <td>-3.915883</td>\n",
       "      <td>9.282379</td>\n",
       "      <td>-11.662352</td>\n",
       "      <td>-0.297657</td>\n",
       "      <td>6.850392</td>\n",
       "      <td>0.169173</td>\n",
       "      <td>-7.042653</td>\n",
       "      <td>3.849116</td>\n",
       "      <td>-5.604168</td>\n",
       "      <td>1.933462</td>\n",
       "      <td>8.454243</td>\n",
       "      <td>3.372092</td>\n",
       "      <td>1.718647</td>\n",
       "      <td>8.124207</td>\n",
       "      <td>-0.925503</td>\n",
       "      <td>-0.496838</td>\n",
       "      <td>5.301016</td>\n",
       "      <td>-5.080378</td>\n",
       "      <td>6.699970</td>\n",
       "      <td>5.197290</td>\n",
       "      <td>-3.068408</td>\n",
       "      <td>3.365889</td>\n",
       "      <td>6.473870</td>\n",
       "      <td>-3.658548</td>\n",
       "      <td>-4.411255</td>\n",
       "      <td>1.767173</td>\n",
       "      <td>-1.339562</td>\n",
       "      <td>-8.309244</td>\n",
       "      <td>2.841215</td>\n",
       "      <td>-3.389058</td>\n",
       "      <td>8.038210</td>\n",
       "      <td>6.272835</td>\n",
       "      <td>-2.217616</td>\n",
       "      <td>-2.993463</td>\n",
       "      <td>0.266628</td>\n",
       "      <td>6.416503</td>\n",
       "      <td>3.439421</td>\n",
       "      <td>7.899129</td>\n",
       "      <td>4.070357</td>\n",
       "      <td>5.386213</td>\n",
       "      <td>-6.423323</td>\n",
       "      <td>-6.905014</td>\n",
       "      <td>0.119183</td>\n",
       "      <td>-6.498280</td>\n",
       "      <td>1.457731</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "81684    1.452379 -1.002719  1.369243  0.701099 -0.009637  0.500139  0.496532   \n",
       "157911   1.486251 -0.967420  1.536840  0.637121 -0.085225  0.618029  0.512894   \n",
       "80826    0.803646 -0.488392  0.661941  0.344817 -0.123173  0.282972  0.167541   \n",
       "20015    1.379577 -1.019494  1.305764  0.618841 -0.104208  0.564617  0.473198   \n",
       "122937  10.063936 -4.303458  9.384829  3.504562  0.041939  2.237945  3.282542   \n",
       "\n",
       "               7         8         9        10         11        12  \\\n",
       "81684   0.047666 -0.210582  0.140533 -0.276183   1.958663  0.165886   \n",
       "157911  0.187284 -0.282539  0.121434 -0.292126   1.878531 -0.079271   \n",
       "80826   0.067699 -0.077539  0.079486 -0.049094   1.067595  0.090799   \n",
       "20015   0.031544 -0.245656  0.225477 -0.219041   1.846485  0.052613   \n",
       "122937  2.435402 -1.531437 -0.442141 -2.631069  10.499796  0.756974   \n",
       "\n",
       "               13        14        15        16        17        18        19  \\\n",
       "81684    1.583723  0.166616 -0.836239  0.514721  0.961807 -0.471568  1.212610   \n",
       "157911   1.632847  0.122785 -0.999917  0.535181  0.820863 -0.416541  1.166093   \n",
       "80826    0.882457  0.137043 -0.496857  0.337070  0.641052 -0.292046  0.811200   \n",
       "20015    1.435924  0.116540 -0.822238  0.438163  0.861800 -0.451325  1.268867   \n",
       "122937  10.981818  0.474040 -8.082710  2.143572  4.524682 -2.882796  5.800526   \n",
       "\n",
       "              20        21        22        23         24        25        26  \\\n",
       "81684  -0.763469 -0.078843  0.736094  0.379265  -1.554926 -0.072742 -0.238671   \n",
       "157911 -0.848064 -0.000088  0.632828  0.377507  -1.843385 -0.022893 -0.185876   \n",
       "80826  -0.236791  0.003213  0.393449  0.225089  -0.667992 -0.178938 -0.119999   \n",
       "20015  -0.604564  0.083039  0.622512  0.211039  -1.334945 -0.199346 -0.310774   \n",
       "122937 -7.650096 -1.994673  6.436169  4.224161 -13.470854  0.714606 -0.428849   \n",
       "\n",
       "              27        28        29        30         31        32        33  \\\n",
       "81684  -1.151327  0.656669  0.281587  0.116646   1.365107  0.310523 -0.693532   \n",
       "157911 -1.058074  0.740442  0.227765 -0.049946   1.566132  0.183009 -0.792614   \n",
       "80826  -0.575368  0.351424  0.131639  0.148536   0.624327  0.154565 -0.262047   \n",
       "20015  -1.015583  0.679428  0.171515  0.252509   1.138689  0.168829 -0.588593   \n",
       "122937 -4.408454  4.168980  1.539393 -1.285654  13.198402  1.674970 -5.881006   \n",
       "\n",
       "              34        35         36        37        38        39        40  \\\n",
       "81684  -0.233098 -1.133831   1.514880  1.557180  0.439971 -0.984416  0.313255   \n",
       "157911 -0.269539 -1.121393   1.605000  1.561734  0.501031 -1.088469  0.518020   \n",
       "80826  -0.109486 -0.540456   0.718081  0.769104  0.293621 -0.456438  0.270199   \n",
       "20015  -0.119247 -0.956236   1.285724  1.466012  0.428442 -0.840155  0.377839   \n",
       "122937 -1.104560 -8.885115  11.447929  9.250537  4.232334 -6.715809  2.645887   \n",
       "\n",
       "              41        42        43        44        45        46        47  \\\n",
       "81684  -1.054495  0.065787 -1.041578  0.363855  0.437784 -1.192662 -0.571464   \n",
       "157911 -0.770219 -0.042091 -0.932183  0.285804  0.455504 -1.144219 -0.475844   \n",
       "80826  -0.500269  0.113389 -0.557005  0.321575  0.222165 -0.693677 -0.222910   \n",
       "20015  -0.834500  0.170125 -0.991207  0.271994  0.288017 -1.048971 -0.446393   \n",
       "122937 -5.093472 -0.301643 -5.834190  0.130726  5.268610 -6.808812 -2.546334   \n",
       "\n",
       "              48        49        50        51        52        53        54  \\\n",
       "81684  -1.302940 -1.093599 -0.601988 -0.941053  0.137043  0.144300  0.932341   \n",
       "157911 -1.203283 -1.177307 -0.642624 -0.950677  0.118128  0.020213  0.876312   \n",
       "80826  -0.692264 -0.647913 -0.317627 -0.426342  0.084019  0.029069  0.485115   \n",
       "20015  -1.120854 -1.094619 -0.496792 -0.786371  0.149387  0.139309  0.981824   \n",
       "122937 -7.562334 -8.913692 -4.610030 -6.939877  0.590639  2.077278  3.614421   \n",
       "\n",
       "              55        56        57        58        59        60        61  \\\n",
       "81684   0.910831 -0.089672 -0.400760  0.397046  0.074110  0.621118 -0.671983   \n",
       "157911  1.092227  0.013274 -0.335458  0.444384 -0.040574  0.669704 -0.594840   \n",
       "80826   0.560512  0.034003 -0.427284  0.229681 -0.011451  0.251503 -0.282460   \n",
       "20015   0.879634  0.020316 -0.535410  0.326228  0.047893  0.383449 -0.653878   \n",
       "122937  7.006286  0.051536 -0.961448  2.701857 -1.155196  5.638363 -2.273206   \n",
       "\n",
       "              62        63         64        65        66        67        68  \\\n",
       "81684  -0.392577  0.788927  -1.376851  0.628460  0.051426  1.232759 -0.262301   \n",
       "157911 -0.161697  0.816085  -1.427242  0.465762 -0.071908  1.336919 -0.481902   \n",
       "80826  -0.191643  0.315873  -0.789709  0.421840  0.089766  0.596190 -0.186518   \n",
       "20015  -0.252492  0.723517  -1.177697  0.655013  0.065887  1.097127 -0.318590   \n",
       "122937 -0.108356  6.335777 -10.119603  3.371395 -1.579070  7.743876 -2.859392   \n",
       "\n",
       "              69        70        71        72        73        74        75  \\\n",
       "81684  -0.109760  0.451482  1.293782  1.526669 -0.739702  1.279637 -0.954438   \n",
       "157911 -0.183875  0.553800  1.406928  1.468062 -0.789349  1.082343 -1.148844   \n",
       "80826  -0.073099  0.161928  0.556822  0.702570 -0.321393  0.768069 -0.469178   \n",
       "20015  -0.144530  0.351692  1.148292  1.315205 -0.691845  1.205920 -1.002308   \n",
       "122937 -0.832090  4.649530  9.663132  8.414061 -4.617828  6.558440 -7.892317   \n",
       "\n",
       "              76        77        78        79        80        81        82  \\\n",
       "81684  -1.503910  0.547250  0.389500 -0.819997  0.309607  0.136138  1.451662   \n",
       "157911 -1.371884  0.501040  0.565724 -0.782709  0.299454  0.212317  1.365652   \n",
       "80826  -0.743509  0.291852  0.237719 -0.489482  0.064605 -0.026531  0.735572   \n",
       "20015  -1.385365  0.516487  0.297656 -0.711724  0.227522  0.093147  1.306324   \n",
       "122937 -9.259053  2.537635  4.648840 -4.541659  2.142683  1.719665  6.481392   \n",
       "\n",
       "              83        84         85        86        87        88        89  \\\n",
       "81684  -0.515017  1.303103  -1.265178 -0.071492  1.104644 -0.209296 -1.004866   \n",
       "157911 -0.572387  1.318753  -1.526228  0.106109  0.931194 -0.150863 -0.925325   \n",
       "80826  -0.261231  0.779453  -0.653346 -0.110785  0.519304 -0.089215 -0.430857   \n",
       "20015  -0.535595  1.261416  -1.122192 -0.142971  0.971361 -0.230418 -0.899607   \n",
       "122937 -3.915883  9.282379 -11.662352 -0.297657  6.850392  0.169173 -7.042653   \n",
       "\n",
       "              90        91        92        93        94        95        96  \\\n",
       "81684   0.687148 -0.979767  0.651393  0.722044  0.673126  0.200752  0.862134   \n",
       "157911  0.741164 -0.925167  0.455999  0.938158  0.687450  0.229408  0.893249   \n",
       "80826   0.354168 -0.535983  0.404685  0.249762  0.344466  0.179717  0.351801   \n",
       "20015   0.666209 -0.949601  0.673725  0.603956  0.534082  0.169462  0.608821   \n",
       "122937  3.849116 -5.604168  1.933462  8.454243  3.372092  1.718647  8.124207   \n",
       "\n",
       "              97        98        99       100       101       102       103  \\\n",
       "81684  -0.239570 -0.529020  0.847729 -0.693538  0.878455  1.047674 -0.483992   \n",
       "157911 -0.257193 -0.385989  0.812355 -0.798476  0.873031  0.889743 -0.514877   \n",
       "80826  -0.038659 -0.317754  0.359617 -0.409534  0.369182  0.594447 -0.228245   \n",
       "20015  -0.065431 -0.614474  0.762683 -0.639805  0.844612  1.073392 -0.363619   \n",
       "122937 -0.925503 -0.496838  5.301016 -5.080378  6.699970  5.197290 -3.068408   \n",
       "\n",
       "             104       105       106       107       108       109       110  \\\n",
       "81684   0.560848  0.893744 -0.238188 -0.399435  0.489400 -0.593171 -0.778336   \n",
       "157911  0.657277  1.003032 -0.246083 -0.497816  0.403705 -0.359008 -0.929250   \n",
       "80826   0.258842  0.449706 -0.103915 -0.230782  0.167989 -0.340933 -0.405068   \n",
       "20015   0.610556  0.759537 -0.140858 -0.438278  0.367778 -0.602195 -0.643786   \n",
       "122937  3.365889  6.473870 -3.658548 -4.411255  1.767173 -1.339562 -8.309244   \n",
       "\n",
       "             111       112       113       114       115       116       117  \\\n",
       "81684   0.281279 -0.629935  0.955278  0.598631 -0.127998 -0.443079 -0.050284   \n",
       "157911  0.235381 -0.533468  1.035063  0.751790 -0.218244 -0.332239 -0.144424   \n",
       "80826   0.055126 -0.334929  0.560386  0.231980 -0.033686 -0.179390  0.002565   \n",
       "20015   0.116179 -0.678912  0.837203  0.478550 -0.078266 -0.311366  0.018070   \n",
       "122937  2.841215 -3.389058  8.038210  6.272835 -2.217616 -2.993463  0.266628   \n",
       "\n",
       "             118       119       120       121       122       123       124  \\\n",
       "81684   1.093501  0.555158  0.729942  0.664189  0.270384 -0.758777 -0.777352   \n",
       "157911  1.094039  0.634521  0.907201  0.700801  0.408076 -0.738221 -1.055755   \n",
       "80826   0.673102  0.371692  0.450679  0.268195  0.135310 -0.367949 -0.316786   \n",
       "20015   1.063512  0.731462  0.656659  0.547255  0.151091 -0.598321 -0.653324   \n",
       "122937  6.416503  3.439421  7.899129  4.070357  5.386213 -6.423323 -6.905014   \n",
       "\n",
       "             125       126       127  TARGET  \n",
       "81684   0.108012 -0.311805 -0.354702     286  \n",
       "157911  0.068182 -0.518598 -0.114026     338  \n",
       "80826  -0.049914 -0.076540 -0.202827     913  \n",
       "20015  -0.005014 -0.338659 -0.403436     182  \n",
       "122937  0.119183 -6.498280  1.457731     231  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(os.path.join(output_dir, \"emb_train.csv\"), header=None).sample(frac=0.5)\n",
    "df_valid = pd.read_csv(os.path.join(output_dir, \"emb_valid.csv\"), header=None)\n",
    "print(\"train\", df_train.shape)\n",
    "print(\"valid\", df_valid.shape)\n",
    "df_train.columns = df_train.columns.tolist()[:-1] + [\"TARGET\"]\n",
    "df_valid.columns = df_valid.columns.tolist()[:-1] + [\"TARGET\"]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_train.columns.tolist()[:-1]\n",
    "n_classes = df_train[\"TARGET\"].nunique()\n",
    "labels_valid = df_valid[\"TARGET\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | learni... | max_depth | min_ch... | min_sp... | num_le... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[1]\tvalid_0's multi_logloss: 6.95514\tvalid_1's multi_logloss: 5.44262\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tvalid_0's multi_logloss: 6.80635\tvalid_1's multi_logloss: 5.3036\n",
      "[3]\tvalid_0's multi_logloss: 6.68774\tvalid_1's multi_logloss: 5.24171\n",
      "[4]\tvalid_0's multi_logloss: 6.58884\tvalid_1's multi_logloss: 5.19734\n",
      "[5]\tvalid_0's multi_logloss: 6.50376\tvalid_1's multi_logloss: 5.18354\n",
      "[6]\tvalid_0's multi_logloss: 6.42877\tvalid_1's multi_logloss: 5.16884\n",
      "[7]\tvalid_0's multi_logloss: 6.36152\tvalid_1's multi_logloss: 5.1744\n",
      "[8]\tvalid_0's multi_logloss: 6.30018\tvalid_1's multi_logloss: 5.17054\n",
      "[9]\tvalid_0's multi_logloss: 6.24432\tvalid_1's multi_logloss: 5.1803\n",
      "[10]\tvalid_0's multi_logloss: 6.19175\tvalid_1's multi_logloss: 5.17903\n",
      "[11]\tvalid_0's multi_logloss: 6.14309\tvalid_1's multi_logloss: 5.18969\n",
      "[12]\tvalid_0's multi_logloss: 6.09721\tvalid_1's multi_logloss: 5.19335\n"
     ]
    }
   ],
   "source": [
    "def accuracy(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    pred = np.argmax(preds.reshape(n_classes, len(preds)//n_classes), axis=0)\n",
    "    return 'accuracy', np.mean(labels == pred), True\n",
    "\n",
    "def lgbm_evaluate(**params):\n",
    "    start = time.time()\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "        \n",
    "    clf = LGBMClassifier(**params, \n",
    "                         n_estimators=10000, \n",
    "                         n_jobs=os.cpu_count(),\n",
    "                         objective=\"multiclass\",\n",
    "                         num_class=n_classes,\n",
    "                        )\n",
    "        \n",
    "    clf.fit(df_train[features].values, df_train[\"TARGET\"].values, \n",
    "            eval_set = [(df_train[features].values, df_train[\"TARGET\"].values),\n",
    "                        (df_valid[features].values, df_valid[\"TARGET\"].values)],\n",
    "            early_stopping_rounds=10, verbose=1)\n",
    "    \n",
    "#     train_preds = clf.predict_proba(df_train[features].values, num_iteration=clf.best_iteration_)\n",
    "    valid_preds = clf.predict_proba(df_valid[features].values, num_iteration=clf.best_iteration_)\n",
    "    \n",
    "#     print('Accuracy train {:.6f}'.format(sum(np.argmax(train_preds, axis=1) == df_train['TARGET'].values) / float(len(train_preds))))\n",
    "    acc_valid = np.mean(np.argmax(valid_preds, axis=1) == df_valid['TARGET'].values)\n",
    "        \n",
    "    return acc_valid\n",
    "\n",
    "def optimize_lgbm():\n",
    "    \n",
    "    params_space = {'colsample_bytree': (0.9, 1.0),\n",
    "                    'learning_rate': (0.01, 1.0), \n",
    "                    'num_leaves': (20, 1000), \n",
    "                    'subsample': (0.5, 1.0), \n",
    "                    'max_depth': (2, 1000), \n",
    "                    'reg_alpha': (0.0, 1.0), \n",
    "                    'reg_lambda': (0.0, 1.0), \n",
    "                    'min_split_gain': (0.0001, 1.),\n",
    "                    'min_child_weight': (5., 200.),\n",
    "                   }\n",
    "\n",
    "    bo = BayesianOptimization(lgbm_evaluate, params_space)\n",
    "    bo.maximize(init_points = 1, n_iter = 1)\n",
    "    \n",
    "    best_acc = bo.max['target']\n",
    "    best_params = bo.max['params']\n",
    "    best_params['num_leaves'] = int(best_params['num_leaves'])\n",
    "    best_params['max_depth'] = int(best_params['max_depth'])\n",
    "    \n",
    "    \n",
    "    print(\"Best validation acc: {}\".format(best_acc))\n",
    "    print('Best parameters found by optimization:\\n')\n",
    "    for k, v in best_params.items():\n",
    "        print(color.BLUE + k + color.END + ' = ' + color.BOLD + str(v)+ color.END + '     [',params_space[k],']')\n",
    "        \n",
    "    return best_acc, best_params\n",
    "\n",
    "best_acc, best_params = optimize_lgbm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------\n",
      "\n",
      "kfolded lightGBM\n",
      "\n",
      "Train set shape: (393420, 128)\n",
      "Valid set shape:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/var/python/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2016, 128)\n",
      "Number of features: 128\n",
      "[1]\ttraining's multi_logloss: 6.94317\ttraining's accuracy: 0.0142519\tvalid_1's multi_logloss: 5.68881\tvalid_1's accuracy: 0.121528\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's multi_logloss: 6.91078\ttraining's accuracy: 0.0209649\tvalid_1's multi_logloss: 5.65548\tvalid_1's accuracy: 0.167163\n",
      "[3]\ttraining's multi_logloss: 6.87927\ttraining's accuracy: 0.0262137\tvalid_1's multi_logloss: 5.62315\tvalid_1's accuracy: 0.176587\n",
      "[4]\ttraining's multi_logloss: 6.84904\ttraining's accuracy: 0.0296985\tvalid_1's multi_logloss: 5.59527\tvalid_1's accuracy: 0.186012\n",
      "[5]\ttraining's multi_logloss: 6.8195\ttraining's accuracy: 0.0320599\tvalid_1's multi_logloss: 5.56562\tvalid_1's accuracy: 0.189484\n",
      "[6]\ttraining's multi_logloss: 6.79073\ttraining's accuracy: 0.0333715\tvalid_1's multi_logloss: 5.53707\tvalid_1's accuracy: 0.194444\n",
      "[7]\ttraining's multi_logloss: 6.76224\ttraining's accuracy: 0.0348254\tvalid_1's multi_logloss: 5.50895\tvalid_1's accuracy: 0.201885\n",
      "[8]\ttraining's multi_logloss: 6.7347\ttraining's accuracy: 0.0357786\tvalid_1's multi_logloss: 5.48444\tvalid_1's accuracy: 0.203373\n",
      "[9]\ttraining's multi_logloss: 6.70776\ttraining's accuracy: 0.0366301\tvalid_1's multi_logloss: 5.45931\tvalid_1's accuracy: 0.204861\n",
      "[10]\ttraining's multi_logloss: 6.68123\ttraining's accuracy: 0.037113\tvalid_1's multi_logloss: 5.43642\tvalid_1's accuracy: 0.209821\n",
      "[11]\ttraining's multi_logloss: 6.6552\ttraining's accuracy: 0.037629\tvalid_1's multi_logloss: 5.41413\tvalid_1's accuracy: 0.214782\n",
      "[12]\ttraining's multi_logloss: 6.62953\ttraining's accuracy: 0.038328\tvalid_1's multi_logloss: 5.39234\tvalid_1's accuracy: 0.215774\n",
      "[13]\ttraining's multi_logloss: 6.60434\ttraining's accuracy: 0.0388008\tvalid_1's multi_logloss: 5.3735\tvalid_1's accuracy: 0.21379\n",
      "[14]\ttraining's multi_logloss: 6.57961\ttraining's accuracy: 0.0393218\tvalid_1's multi_logloss: 5.35451\tvalid_1's accuracy: 0.219246\n",
      "[15]\ttraining's multi_logloss: 6.55526\ttraining's accuracy: 0.039909\tvalid_1's multi_logloss: 5.33649\tvalid_1's accuracy: 0.220238\n",
      "[16]\ttraining's multi_logloss: 6.53122\ttraining's accuracy: 0.0402801\tvalid_1's multi_logloss: 5.31793\tvalid_1's accuracy: 0.22123\n",
      "[17]\ttraining's multi_logloss: 6.50762\ttraining's accuracy: 0.0407554\tvalid_1's multi_logloss: 5.30143\tvalid_1's accuracy: 0.219246\n",
      "[18]\ttraining's multi_logloss: 6.48436\ttraining's accuracy: 0.0412562\tvalid_1's multi_logloss: 5.28421\tvalid_1's accuracy: 0.220238\n",
      "[19]\ttraining's multi_logloss: 6.46145\ttraining's accuracy: 0.041701\tvalid_1's multi_logloss: 5.26847\tvalid_1's accuracy: 0.220734\n",
      "[20]\ttraining's multi_logloss: 6.43913\ttraining's accuracy: 0.0422398\tvalid_1's multi_logloss: 5.25294\tvalid_1's accuracy: 0.222222\n",
      "[21]\ttraining's multi_logloss: 6.41684\ttraining's accuracy: 0.0428524\tvalid_1's multi_logloss: 5.23901\tvalid_1's accuracy: 0.22123\n",
      "[22]\ttraining's multi_logloss: 6.39484\ttraining's accuracy: 0.0433684\tvalid_1's multi_logloss: 5.22462\tvalid_1's accuracy: 0.222718\n",
      "[23]\ttraining's multi_logloss: 6.37337\ttraining's accuracy: 0.0439429\tvalid_1's multi_logloss: 5.21079\tvalid_1's accuracy: 0.223214\n",
      "[24]\ttraining's multi_logloss: 6.35216\ttraining's accuracy: 0.0445732\tvalid_1's multi_logloss: 5.1981\tvalid_1's accuracy: 0.223214\n",
      "[25]\ttraining's multi_logloss: 6.33121\ttraining's accuracy: 0.0451451\tvalid_1's multi_logloss: 5.18471\tvalid_1's accuracy: 0.226687\n",
      "[26]\ttraining's multi_logloss: 6.31064\ttraining's accuracy: 0.0456967\tvalid_1's multi_logloss: 5.17244\tvalid_1's accuracy: 0.224702\n",
      "[27]\ttraining's multi_logloss: 6.29016\ttraining's accuracy: 0.0462991\tvalid_1's multi_logloss: 5.16061\tvalid_1's accuracy: 0.227183\n",
      "[28]\ttraining's multi_logloss: 6.27008\ttraining's accuracy: 0.0469015\tvalid_1's multi_logloss: 5.14907\tvalid_1's accuracy: 0.227183\n",
      "[29]\ttraining's multi_logloss: 6.25025\ttraining's accuracy: 0.0475065\tvalid_1's multi_logloss: 5.13851\tvalid_1's accuracy: 0.228671\n",
      "[30]\ttraining's multi_logloss: 6.23055\ttraining's accuracy: 0.0481165\tvalid_1's multi_logloss: 5.12713\tvalid_1's accuracy: 0.226687\n",
      "[31]\ttraining's multi_logloss: 6.21119\ttraining's accuracy: 0.0487647\tvalid_1's multi_logloss: 5.11736\tvalid_1's accuracy: 0.229167\n",
      "[32]\ttraining's multi_logloss: 6.19201\ttraining's accuracy: 0.0495018\tvalid_1's multi_logloss: 5.10722\tvalid_1's accuracy: 0.226687\n",
      "[33]\ttraining's multi_logloss: 6.17308\ttraining's accuracy: 0.0502008\tvalid_1's multi_logloss: 5.09741\tvalid_1's accuracy: 0.22371\n",
      "[34]\ttraining's multi_logloss: 6.15452\ttraining's accuracy: 0.0507702\tvalid_1's multi_logloss: 5.08784\tvalid_1's accuracy: 0.221726\n",
      "[35]\ttraining's multi_logloss: 6.13603\ttraining's accuracy: 0.0516496\tvalid_1's multi_logloss: 5.07997\tvalid_1's accuracy: 0.221726\n",
      "[36]\ttraining's multi_logloss: 6.11767\ttraining's accuracy: 0.052369\tvalid_1's multi_logloss: 5.07169\tvalid_1's accuracy: 0.218254\n",
      "[37]\ttraining's multi_logloss: 6.09957\ttraining's accuracy: 0.0532662\tvalid_1's multi_logloss: 5.06333\tvalid_1's accuracy: 0.215774\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-e87d9f737f9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m }\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlightgbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-e87d9f737f9f>\u001b[0m in \u001b[0;36mlightgbm\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'...model trained'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/var/python/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid_sets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m             \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/var/python/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36meval_train\u001b[0;34m(self, feval)\u001b[0m\n\u001b[1;32m   1956\u001b[0m             \u001b[0mList\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \"\"\"\n\u001b[0;32m-> 1958\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__inner_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__train_data_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/var/python/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__inner_eval\u001b[0;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[1;32m   2362\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2363\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_out_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2364\u001b[0;31m                 result.ctypes.data_as(ctypes.POINTER(ctypes.c_double))))\n\u001b[0m\u001b[1;32m   2365\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtmp_out_len\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_inner_eval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2366\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wrong length of eval results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "def lightgbm(params):\n",
    "    \n",
    "    print('\\n--------------------------------------------\\n')\n",
    "    print('kfolded lightGBM\\n')\n",
    "    \n",
    "    print('Train set shape:', dtrain.data.shape)\n",
    "    print('Valid set shape:', dvalid.data.shape)\n",
    "\n",
    "#     # Create arrays and dataframes to store results\n",
    "#     oof_preds = np.zeros(df_train.shape[0])\n",
    "#     sub_preds = np.zeros(df_test.shape[0])\n",
    "#     df_feature_importance = pd.DataFrame()\n",
    "    \n",
    "    print('Number of features: {}'.format(len(features)))\n",
    "                    \n",
    "    def accuracy(preds, train_data):\n",
    "        labels = train_data.get_label()\n",
    "        pred = np.argmax(preds.reshape(n_classes, len(preds)//n_classes), axis=0)\n",
    "        return 'accuracy', np.mean(labels == pred), True\n",
    "    \n",
    "    clf = lgb.train(\n",
    "        params=params,\n",
    "        train_set=dtrain,\n",
    "#         num_boost_round=10000,\n",
    "        valid_sets=[dtrain, dvalid],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=True,\n",
    "        feval=accuracy,\n",
    "    )\n",
    "    print('...model trained')\n",
    "\n",
    "    train_preds = clf.predict(dtrain.data)\n",
    "    valid_preds = clf.predict(dvalid.data)\n",
    "    print('...predictions made')\n",
    "    print('Accuracy train {:.6f}'.format(np.mean(np.argmax(train_preds, axis=1) == df_train['TARGET'].values)) )\n",
    "    print('Accuracy valid {:.6f}'.format(np.mean(np.argmax(valid_preds, axis=1) == df_valid['TARGET'].values)) )\n",
    "    return clf\n",
    "\n",
    "params = {  \"objective\" : \"multiclass\",\n",
    "            \"num_class\" : n_classes,\n",
    "            'n_estimators': 10000,\n",
    "            'learning_rate': .02,\n",
    "            'num_leaves': 1000,\n",
    "            'colsample_bytree': 1.,\n",
    "            'subsample': 1.,\n",
    "            'max_depth': 100,\n",
    "            'reg_alpha': .041545473,\n",
    "            'reg_lambda': .0735294,\n",
    "            'min_split_gain': .0222415,\n",
    "            'min_child_weight': 39.3259775,                \n",
    "#             \"device_type\" : \"gpu\",\n",
    "            \"njobs\" : os.cpu_count(),\n",
    "}\n",
    "\n",
    "clf = lightgbm(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = clf.predict(dtrain.data)\n",
    "valid_preds = clf.predict(dvalid.data)\n",
    "train_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39342,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(train_preds, axis=1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
